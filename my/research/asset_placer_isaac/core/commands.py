# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES.
# All rights reserved.
# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
#
# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
# property and proprietary rights in and to this material, related
# documentation and any modifications thereto. Any use, reproduction,
# disclosure or distribution of this material and related documentation
# without an express license agreement from NVIDIA CORPORATION or
# its affiliates is strictly prohibited.

import asyncio
import datetime
import json
import math
import os
import traceback
from typing import Any, Dict, List, Optional, Tuple

import omni.kit.app
import omni.log
import omni.usd
from pxr import Gf, Sdf, Tf, Usd, UsdGeom

from .. import backend as be
from .constants import (
    ADV_MODEL_CHOICES,
    DEFAULT_PROMPT1_TEXT,
    DEFAULT_PROMPT2_TEXT,
    IMAGE_DETAIL_CHOICES,
    JSON_SIZE_MODE_CHOICES,
    MODEL_CHOICES,
    REASONING_EFFORT_CHOICES,
    TEXT_VERBOSITY_CHOICES,
    VECTOR_SEARCH_LIMIT,
)


class CommandsMixin:
    def _get_ai_runtime_options(self, base_model_name: str) -> Tuple[str, str, Dict[str, object]]:
        step1_combo = getattr(self, "_ai_step1_model_combo", None)
        step2_combo = getattr(self, "_ai_step2_model_combo", None)
        effort_combo = getattr(self, "_ai_reasoning_effort_combo", None)
        verbosity_combo = getattr(self, "_ai_text_verbosity_combo", None)
        detail_combo = getattr(self, "_ai_image_detail_combo", None)

        step1_index = step1_combo.model.get_item_value_model().as_int if step1_combo else self._ai_step1_model_index
        step2_index = step2_combo.model.get_item_value_model().as_int if step2_combo else self._ai_step2_model_index
        effort_index = effort_combo.model.get_item_value_model().as_int if effort_combo else self._ai_reasoning_effort_index
        verbosity_index = (
            verbosity_combo.model.get_item_value_model().as_int if verbosity_combo else self._ai_text_verbosity_index
        )
        detail_index = detail_combo.model.get_item_value_model().as_int if detail_combo else self._ai_image_detail_index

        step1_model_name = base_model_name
        step2_model_name = base_model_name
        if 0 < step1_index < len(ADV_MODEL_CHOICES):
            step1_model_name = ADV_MODEL_CHOICES[step1_index]
        if 0 < step2_index < len(ADV_MODEL_CHOICES):
            step2_model_name = ADV_MODEL_CHOICES[step2_index]

        reasoning_effort = None
        if 0 < effort_index < len(REASONING_EFFORT_CHOICES):
            reasoning_effort = REASONING_EFFORT_CHOICES[effort_index]

        text_verbosity = None
        if 0 < verbosity_index < len(TEXT_VERBOSITY_CHOICES):
            text_verbosity = TEXT_VERBOSITY_CHOICES[verbosity_index]

        image_detail = None
        if 0 < detail_index < len(IMAGE_DETAIL_CHOICES):
            image_detail = IMAGE_DETAIL_CHOICES[detail_index]

        max_output_tokens = self._ai_max_output_tokens
        if hasattr(self, "_ai_max_output_tokens_model"):
            raw_tokens = self._ai_max_output_tokens_model.as_string.strip()
            if raw_tokens:
                try:
                    max_output_tokens = int(float(raw_tokens))
                except (ValueError, TypeError):
                    max_output_tokens = self._ai_max_output_tokens
        if max_output_tokens <= 0:
            max_output_tokens = None

        return step1_model_name, step2_model_name, {
            "reasoning_effort": reasoning_effort,
            "max_output_tokens": max_output_tokens,
            "text_verbosity": text_verbosity,
            "image_detail": image_detail,
        }

    def _get_selected_json_size_mode(self) -> str:
        default_index = 1 if len(JSON_SIZE_MODE_CHOICES) > 1 else 0
        index = getattr(self, "_json_size_mode_index", default_index)
        combo = getattr(self, "_json_size_mode_combo", None)
        if combo:
            try:
                index = combo.model.get_item_value_model().as_int
            except Exception:
                index = getattr(self, "_json_size_mode_index", default_index)
        if not isinstance(index, int):
            index = default_index
        if index < 0 or index >= len(JSON_SIZE_MODE_CHOICES):
            index = default_index
        self._json_size_mode_index = index

        mode = str(JSON_SIZE_MODE_CHOICES[index]).lower()
        if mode not in ("world", "local"):
            mode = "world"
        return mode

    def _apply_size_mode_override_to_prompt2(self, prompt2_text: str, size_mode: str) -> str:
        mode = str(size_mode or "world").lower()
        if mode not in ("world", "local"):
            mode = "world"

        if mode == "local":
            override_block = (
                "\n\n[SIZE_MODE OVERRIDE]\n"
                "This override has highest priority over any previous size definition.\n"
                "Use top-level `size_mode` = \"local\".\n"
                "Length = object local +X (right), Width = object local +Y (forward / functional front), Height = local +Z.\n"
                "rotationZ defines where object local +Y points in world (0:+Y, 90:+X, 180:-Y, 270:-X).\n"
                "Do NOT swap Length/Width based on rotationZ.\n"
            )
        else:
            override_block = (
                "\n\n[SIZE_MODE OVERRIDE]\n"
                "This override has highest priority over any previous size definition.\n"
                "Use top-level `size_mode` = \"world\".\n"
                "Length = world X size, Width = world Y size, Height = world Z size.\n"
                "rotationZ still follows functional front direction (0:+Y, 90:+X, 180:-Y, 270:-X).\n"
                "In world mode, Length/Width values must represent final world-axis footprint.\n"
            )
        return f"{prompt2_text.rstrip()}{override_block}"

    def _enforce_size_mode_on_layout(self, layout_json: Any, size_mode: str) -> Any:
        if not isinstance(layout_json, dict):
            return layout_json
        mode = str(size_mode or "world").lower()
        if mode not in ("world", "local"):
            mode = "world"
        layout_json["size_mode"] = mode
        return layout_json

    def _get_replacement_history(self, prim_path: str) -> Dict[str, List[str]]:
        if not hasattr(self, "_replacement_history_by_prim") or self._replacement_history_by_prim is None:
            self._replacement_history_by_prim = {}
        history = self._replacement_history_by_prim.get(prim_path)
        if not history:
            history = {"urls": [], "keys": []}
            self._replacement_history_by_prim[prim_path] = history
        return history


    def _append_replacement_history(
        self, prim_path: str, asset_url: Optional[str], identity_key: Optional[str]
    ) -> None:
        limit = int(getattr(self, "_replacement_history_limit", 20))
        if limit <= 0:
            return
        history = self._get_replacement_history(prim_path)
        if asset_url:
            normalized = self._normalize_asset_url(asset_url)
            if normalized and normalized not in history["urls"]:
                history["urls"].append(normalized)
                if len(history["urls"]) > limit:
                    history["urls"] = history["urls"][-limit:]
        if identity_key and identity_key not in history["keys"]:
            history["keys"].append(identity_key)
            if len(history["keys"]) > limit:
                history["keys"] = history["keys"][-limit:]


    async def _select_next_history_url(
        self, history_urls: List[str], current_url: str
    ) -> Optional[str]:
        if not history_urls or len(history_urls) < 2:
            return None
        try:
            start_idx = history_urls.index(current_url)
        except ValueError:
            start_idx = -1

        total = len(history_urls)
        for offset in range(1, total + 1):
            candidate = history_urls[(start_idx + offset) % total]
            if candidate == current_url:
                continue
            if self._is_asset_blacklisted(candidate):
                continue
            candidate_key = await self._get_asset_identity_key(candidate)
            if candidate_key and candidate_key in getattr(self, "_asset_blacklist_keys", set()):
                continue
            return candidate
        return None


    async def _do_ai_generation(self):
        """
        ï¼ˆéžåŒæœŸï¼‰AIç”Ÿæˆã®ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯

        ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ asyncio.ensure_future() ã«ã‚ˆã£ã¦éžåŒæœŸã‚¿ã‚¹ã‚¯ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹ã€‚
        Resubmitæ™‚ã«ã¯ã€self._additional_context ã«è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã€‚
        """
        try:
            omni.log.info("+"*80)
            omni.log.info("*** _do_ai_generation() STARTED ***")
            omni.log.info("+"*80)
            omni.log.info(f"  Image path: {self._image_path}")
            omni.log.info(f"  Dimensions path: {self._dimensions_path}")
            omni.log.info(f"  Additional context: {len(self._additional_context) if self._additional_context else 0} chars")

            # --- 1. å…¥åŠ›ãƒã‚§ãƒƒã‚¯ ---
            if not self._image_path:
                omni.log.error("âœ— Image file not selected")
                self._analysis_text_model.as_string = "Error: Please select an image file first."
                return

            if not self._dimensions_path:
                omni.log.error("Please select a dimensions file first")
                self._analysis_text_model.as_string = "Error: Please select a dimensions file first."
                return

            # APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆèµ·å‹•æ™‚ã«ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿æ¸ˆã¿ï¼‰
            api_key = self._api_key_model.as_string.strip()

            if not api_key:
                omni.log.error(
                    "OpenAI API Key is not set. Set the OPENAI_API_KEY environment variable or enter it in the UI."
                )
                self._analysis_text_model.as_string = (
                    "Error: OpenAI API Key is not set. Set the OPENAI_API_KEY environment variable or enter it in the UI."
                )
                return

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã®èª­ã¿è¾¼ã¿
            prompt1_text, prompt1_source = self._resolve_prompt_text(
                self._prompt1_path, "prompt_1.txt", DEFAULT_PROMPT1_TEXT, "Prompt 1"
            )
            prompt2_text, prompt2_source = self._resolve_prompt_text(
                self._prompt2_path, "prompt_2.txt", DEFAULT_PROMPT2_TEXT, "Prompt 2"
            )
            json_size_mode = self._get_selected_json_size_mode()
            prompt2_text = self._apply_size_mode_override_to_prompt2(prompt2_text, json_size_mode)
            dimensions_text = self._read_text_with_fallback(self._dimensions_path, "Dimensions", required=True)

            if dimensions_text is None:
                self._analysis_text_model.as_string = "Error: Failed to load dimensions file."
                return

            # ãƒ¢ãƒ‡ãƒ«åã®å–å¾—
            model_index = self._model_combo.model.get_item_value_model().as_int
            if model_index < 0 or model_index >= len(MODEL_CHOICES):
                model_index = 0
            model_name = MODEL_CHOICES[model_index]
            step1_model_name, step2_model_name, ai_overrides = self._get_ai_runtime_options(model_name)

            omni.log.info(f"Using Step 1 model: {step1_model_name}")
            omni.log.info(f"Using Step 2 model: {step2_model_name}")
            omni.log.info(f"Image: {self._image_path}")
            omni.log.info(f"Dimensions: {self._dimensions_path}")
            omni.log.info(f"Prompt 1 source: {prompt1_source}")
            omni.log.info(f"Prompt 2 source: {prompt2_source}")
            omni.log.info(f"JSON size_mode for generation: {json_size_mode}")

            # ç”»åƒã®Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            image_base64 = be.encode_image_to_base64(self._image_path)
            if not image_base64:
                omni.log.error("Failed to encode image to base64")
                self._analysis_text_model.as_string = "Error: Failed to encode image to base64."
                return

            if not prompt1_text or not prompt2_text:
                omni.log.error("Failed to load prompt text. Aborting generation.")
                self._analysis_text_model.as_string = "Error: Failed to load prompt text."
                return

            # ä¿®æ­£æŒ‡ç¤ºãŒã‚ã‚‹å ´åˆã¯prompt1ã«è¿½åŠ 
            if self._additional_context:
                omni.log.info("="*60)
                omni.log.info("ã€RESUBMISSION WITH ADDITIONAL CONTEXTã€‘")
                omni.log.info(f"Context length: {len(self._additional_context)} characters")
                omni.log.info(f"Context preview: {self._additional_context[:200]}...")
                omni.log.info("="*60)
                prompt1_text += f"\n\nã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è¿½åŠ æŒ‡ç¤ºãƒ»ä¿®æ­£ã€‘\n{self._additional_context}"
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé©ç”¨å¾Œã«ã‚¯ãƒªã‚¢ï¼ˆæ¬¡å›žã®å®Ÿè¡Œã§æ··ã–ã‚‰ãªã„ã‚ˆã†ã«ï¼‰
                # self._additional_context = ""  # ã“ã‚Œã¯ã‚„ã‚‰ãªã„ï¼šæ‰¿èªã‚¹ãƒ†ãƒƒãƒ—ã§ä½¿ã†ã‹ã‚‚ã—ã‚Œãªã„

            # --- 2. ã‚¹ãƒ†ãƒƒãƒ—1: ç”»åƒåˆ†æž (éžåŒæœŸå‘¼ã³å‡ºã—) ---
            omni.log.info("=== Step 1: Analyzing image ===")
            analysis_text, step1_stats = await be.step1_analyze_image(
                image_base64,
                prompt1_text,
                dimensions_text,
                step1_model_name,
                api_key,
                **ai_overrides,
            )

            if not analysis_text:
                omni.log.error("Failed to analyze image")
                self._analysis_text_model.as_string = "Error: Failed to analyze image."
                return

            # --- â–¼ UIã«çµæžœã‚’åæ˜  â–¼ ---
            step1_stats_text = f"\\n\\nâ”â”â” Step 1 Stats â”â”â”\\nâ± Time: {step1_stats['time']:.2f}s | ðŸ“¥ Prompt: {step1_stats['prompt_tokens']:,} | ðŸ“¤ Completion: {step1_stats['completion_tokens']:,} | ðŸ“Š Total: {step1_stats['total_tokens']:,}"
            self._analysis_text_model.as_string = f"Step 1 (Analysis) Complete:{step1_stats_text}\\n\\n{analysis_text}"
            self._set_ai_tokens(step1_stats, None)
            self._set_ai_status("AI Status: Step 1 (Analysis) Complete")
            omni.log.info(f"Analysis completed in {step1_stats['time']:.2f}s")
            omni.log.info(f"Analysis result:\n{analysis_text}")

            # æ‰¿èªãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ã“ã“ã§åœæ­¢
            if self._require_approval.as_bool:
                omni.log.info("Approval mode enabled. Waiting for user approval...")
                self._analysis_text_model.as_string += (
                    "\n\nâ¸ï¸ Waiting for approval. Click 'Approve' to continue or 'Reject' to add context."
                )

                # åˆ†æžçµæžœã‚’ä¿å­˜ï¼ˆæ‰¿èªå¾Œã«ä½¿ç”¨ï¼‰
                self._analysis_result = {
                    "analysis_text": analysis_text,
                    "step1_stats": step1_stats,
                    "image_base64": image_base64,
                    "prompt2_text": prompt2_text,
                    "json_size_mode": json_size_mode,
                    "dimensions_text": dimensions_text,
                    "model_name": model_name,
                    "step2_model_name": step2_model_name,
                    "ai_overrides": ai_overrides,
                    "api_key": api_key
                }

                # æ‰¿èªãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
                self._approval_buttons_container.visible = True
                self._approval_pending = True
                return  # ã“ã“ã§ä¸€æ—¦åœæ­¢

            # æ‰¿èªãƒ¢ãƒ¼ãƒ‰ãŒã‚ªãƒ•ã®å ´åˆã¯ç¶šè¡Œ
            omni.log.info("Approval mode disabled. Continuing to Step 2...")

            # --- 3. ã‚¹ãƒ†ãƒƒãƒ—2: JSONç”Ÿæˆ (éžåŒæœŸå‘¼ã³å‡ºã—) ---
            omni.log.info("=== Step 2: Generating JSON ===")
            self._analysis_text_model.as_string += "\n\nGenerating... (Step 2: Creating JSON...)"

            layout_json, step2_stats = await be.step2_generate_json(
                analysis_text,
                dimensions_text,
                image_base64,
                prompt2_text,
                step2_model_name,
                api_key,
                **ai_overrides,
            )

            if not layout_json:
                omni.log.error("Failed to generate JSON")
                self._analysis_text_model.as_string += "\nError: Failed to generate JSON."
                return
            layout_json = self._enforce_size_mode_on_layout(layout_json, json_size_mode)

            self._analysis_text_model.as_string += "\nJSON Generation Complete."
            self._set_ai_tokens(step1_stats, step2_stats)
            self._set_ai_status("AI Status: Step 2 (JSON Generation) Complete")
            omni.log.info(f"JSON generation completed in {step2_stats['time']:.2f}s")

            # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M")
            base_name = os.path.splitext(os.path.basename(self._image_path))[0]
            model_name_safe = step2_model_name.replace("/", "-").replace(":", "-")
            json_filename = f"{base_name}_layout_{model_name_safe}_{timestamp}.json"

            out_dir = self._get_json_output_dir()
            os.makedirs(out_dir, exist_ok=True)
            json_path = os.path.join(out_dir, json_filename)

            try:
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(layout_json, f, indent=4, ensure_ascii=False)
                omni.log.info(f"JSON saved to: {json_path}")
                self._analysis_text_model.as_string += f"\nJSON saved to: {json_path}"
                self._set_generated_json_preview(json_path, layout_json)
            except Exception as e:
                omni.log.error(f"Failed to save JSON: {e}")
                self._analysis_text_model.as_string += f"\nError: Failed to save JSON: {e}"
                return

            # çµ±è¨ˆæƒ…å ±ã®å‡ºåŠ›
            total_time = step1_stats["time"] + step2_stats["time"]
            total_prompt_tokens = step1_stats["prompt_tokens"] + step2_stats["prompt_tokens"]
            total_completion_tokens = step1_stats["completion_tokens"] + step2_stats["completion_tokens"]
            total_tokens = step1_stats["total_tokens"] + step2_stats["total_tokens"]

            omni.log.info("="*60)
            omni.log.info("ã€Total Statistics (Step 1 + Step 2)ã€‘")
            omni.log.info("="*60)
            omni.log.info(f"Total processing time: {total_time:.2f}s")
            omni.log.info(f"Total prompt tokens: {total_prompt_tokens:,}")
            omni.log.info(f"Total completion tokens: {total_completion_tokens:,}")
            omni.log.info(f"Total tokens: {total_tokens:,}")
            omni.log.info("="*60)

            # --- 4. ã‚¹ãƒ†ãƒƒãƒ—3: è¡çªæ¤œå‡º ---
            omni.log.info("=== Step 3: Checking collisions ===")
            total_stats_text = f"\\n\\nâ”â”â” Total Stats (Step 1 + Step 2) â”â”â”\\nâ± Time: {total_time:.2f}s | ðŸ“¥ Prompt: {total_prompt_tokens:,} | ðŸ“¤ Completion: {total_completion_tokens:,} | ðŸ“Š Total: {total_tokens:,}"
            self._analysis_text_model.as_string += total_stats_text
            self._analysis_text_model.as_string += "\n\nChecking collisions..."
            collision_results = be.step3_check_collisions(layout_json)

            # ãƒ­ã‚°ã‚’ä¿å­˜
            log_path = self._save_generation_log(
                step1_stats=step1_stats,
                step2_stats=step2_stats,
                analysis_text=analysis_text,
                layout_json=layout_json,
                collision_results=collision_results,
                model_name=model_name,
                step2_model_name=step2_model_name,
                ai_overrides=ai_overrides,
                image_path=self._image_path,
                prompt1_text=prompt1_text,
                prompt2_text=prompt2_text,
                dimensions_text=dimensions_text
            )
            if log_path:
                self._analysis_text_model.as_string += f"\nDetailed log saved to: {log_path}"

            # --- 5. ã‚¹ãƒ†ãƒƒãƒ—4/5: ã‚¢ã‚»ãƒƒãƒˆé…ç½® ---
            self._layout_json = layout_json
            self._analysis_text_model.as_string += "\nStarting asset placement..."

            # _start_asset_searchã¯ã™ã§ã«éžåŒæœŸã‚¿ã‚¹ã‚¯ã¨ã—ã¦å‹•ä½œ
            self._start_asset_search(self._layout_json)

            self._analysis_text_model.as_string += "\n\nProcess finished."
            omni.log.info("AI JSON generation completed successfully!")

        except asyncio.CancelledError:
            omni.log.warn("AI Generation task was cancelled.")
            self._analysis_text_model.as_string = "Task Cancelled."
            raise
        except Exception as e:
            omni.log.error(f"AI Generation failed: {e}")
            self._analysis_text_model.as_string = f"Error: {e}"
        finally:
            self._set_ai_busy(False)
            self._ai_task = None

    async def _do_step2_and_placement(self):
        """Step 2ï¼ˆJSONç”Ÿæˆï¼‰ä»¥é™ã‚’å®Ÿè¡Œ"""
        try:
            if not self._analysis_result:
                omni.log.error("No analysis result available")
                return

            # ä¿å­˜ã—ãŸåˆ†æžçµæžœã‚’å–å¾—
            analysis_text = self._analysis_result["analysis_text"]
            step1_stats = self._analysis_result["step1_stats"]
            image_base64 = self._analysis_result["image_base64"]
            prompt2_text = self._analysis_result["prompt2_text"]
            json_size_mode = self._analysis_result.get("json_size_mode") or self._get_selected_json_size_mode()
            dimensions_text = self._analysis_result["dimensions_text"]
            model_name = self._analysis_result["model_name"]
            step2_model_name = self._analysis_result.get("step2_model_name", model_name)
            ai_overrides = self._analysis_result.get("ai_overrides", {})
            api_key = self._analysis_result["api_key"]
            omni.log.info(f"JSON size_mode for generation: {json_size_mode}")

            # --- ã‚¹ãƒ†ãƒƒãƒ—2: JSONç”Ÿæˆ (éžåŒæœŸå‘¼ã³å‡ºã—) ---
            omni.log.info("=== Step 2: Generating JSON ===")
            self._analysis_text_model.as_string += "\n\nGenerating... (Step 2: Creating JSON...)"

            layout_json, step2_stats = await be.step2_generate_json(
                analysis_text,
                dimensions_text,
                image_base64,
                prompt2_text,
                step2_model_name,
                api_key,
                **ai_overrides,
            )

            if not layout_json:
                omni.log.error("Failed to generate JSON")
                self._analysis_text_model.as_string += "\nError: Failed to generate JSON."
                return
            layout_json = self._enforce_size_mode_on_layout(layout_json, json_size_mode)

            self._analysis_text_model.as_string += "\nJSON Generation Complete."
            self._set_ai_tokens(self._analysis_result["step1_stats"], step2_stats)
            self._set_ai_status("AI Status: Step 2 (JSON Generation) Complete")
            omni.log.info(f"JSON generation completed in {step2_stats['time']:.2f}s")

            # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M")
            base_name = os.path.splitext(os.path.basename(self._image_path))[0]
            model_name_safe = step2_model_name.replace("/", "-").replace(":", "-")
            json_filename = f"{base_name}_layout_{model_name_safe}_{timestamp}.json"

            out_dir = self._get_json_output_dir()
            os.makedirs(out_dir, exist_ok=True)
            json_path = os.path.join(out_dir, json_filename)

            try:
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(layout_json, f, indent=4, ensure_ascii=False)
                omni.log.info(f"JSON saved to: {json_path}")
                self._analysis_text_model.as_string += f"\nJSON saved to: {json_path}"
                self._set_generated_json_preview(json_path, layout_json)
            except Exception as e:
                omni.log.error(f"Failed to save JSON: {e}")
                self._analysis_text_model.as_string += f"\nError: Failed to save JSON: {e}"
                return

            # çµ±è¨ˆæƒ…å ±ã®å‡ºåŠ›
            total_time = step1_stats["time"] + step2_stats["time"]
            total_prompt_tokens = step1_stats["prompt_tokens"] + step2_stats["prompt_tokens"]
            total_completion_tokens = step1_stats["completion_tokens"] + step2_stats["completion_tokens"]
            total_tokens = step1_stats["total_tokens"] + step2_stats["total_tokens"]

            omni.log.info("="*60)
            omni.log.info("ã€Total Statistics (Step 1 + Step 2)ã€‘")
            omni.log.info("="*60)
            omni.log.info(f"Total processing time: {total_time:.2f}s")
            omni.log.info(f"Total prompt tokens: {total_prompt_tokens:,}")
            omni.log.info(f"Total completion tokens: {total_completion_tokens:,}")
            omni.log.info(f"Total tokens: {total_tokens:,}")
            omni.log.info("="*60)

            # --- ã‚¹ãƒ†ãƒƒãƒ—3: è¡çªæ¤œå‡º ---
            omni.log.info("=== Step 3: Checking collisions ===")
            total_stats_text = f"\\n\\nâ”â”â” Total Stats (Step 1 + Step 2) â”â”â”\\nâ± Time: {total_time:.2f}s | ðŸ“¥ Prompt: {total_prompt_tokens:,} | ðŸ“¤ Completion: {total_completion_tokens:,} | ðŸ“Š Total: {total_tokens:,}"
            self._analysis_text_model.as_string += total_stats_text
            # --- ã‚¹ãƒ†ãƒƒãƒ—3: è¡çªæ¤œå‡º ---
            omni.log.info("=== Step 3: Checking collisions ===")
            collision_results = be.step3_check_collisions(layout_json)

            # ãƒ­ã‚°ã‚’ä¿å­˜
            log_path = self._save_generation_log(
                step1_stats=step1_stats,
                step2_stats=step2_stats,
                analysis_text=analysis_text,
                layout_json=layout_json,
                collision_results=collision_results,
                model_name=model_name,
                step2_model_name=step2_model_name,
                ai_overrides=ai_overrides,
                image_path=self._image_path,
                prompt1_text=prompt2_text, # â€»ã“ã®ã‚±ãƒ¼ã‚¹ã§ã¯Step2ã‹ã‚‰é–‹å§‹
                prompt2_text=prompt2_text,
                dimensions_text=dimensions_text,
                tag="step2_only"
            )
            if log_path:
                self._analysis_text_model.as_string += f"\nDetailed log saved to: {log_path}"

            # --- ã‚¹ãƒ†ãƒƒãƒ—4ä»¥é™: ã‚¢ã‚»ãƒƒãƒˆæ¤œç´¢ã¨é…ç½® ---
            self._layout_json = layout_json
            self._analysis_text_model.as_string += "\n\nStarting asset placement..."
            self._start_asset_search(self._layout_json)
            self._start_asset_search(self._layout_json)

            self._analysis_text_model.as_string += "\n\nProcess finished."
            omni.log.info("Step 2-5 completed successfully!")

        except asyncio.CancelledError:
            omni.log.warn("Step 2-5 task was cancelled.")
            self._analysis_text_model.as_string += "\nTask Cancelled."
            raise
        except Exception as e:
            omni.log.error(f"Step 2-5 failed: {e}")
            self._analysis_text_model.as_string += f"\nError: {e}"
        finally:
            self._set_ai_busy(False)
            self._ai_task = None

    # --- ãƒ•ã‚§ãƒ¼ã‚º4 (å…±é€šã®é–‹å§‹ç‚¹) ---
    def _start_asset_search(self, layout_data: dict):
        """JSONãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¨ã«USD Searchã‚’é–‹å§‹ã™ã‚‹ (ãƒ•ã‚§ãƒ¼ã‚º4)"""
        if layout_data is None:
            omni.log.error("No layout data to process.")
            return

        # ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’Z-Up / ãƒ¡ãƒ¼ãƒˆãƒ«å˜ä½ã«è¨­å®š
        stage = omni.usd.get_context().get_stage()
        if stage:
            UsdGeom.SetStageUpAxis(stage, UsdGeom.Tokens.z)
            UsdGeom.SetStageMetersPerUnit(stage, 1.0)
            omni.log.info("Stage configured: Z-Up, 1.0 meters per unit")

        omni.log.info("="*50)
        omni.log.info("STARTING PHASE 4: USD Search")
        omni.log.info("Layout data received:")
        omni.log.info(f"{layout_data}")
        omni.log.info("="*50)

        if self._search_task and not self._search_task.done():
            self._search_task.cancel()


        # Debug bbox placement is triggered via a dedicated button.

        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’éžåŒæœŸã‚¿ã‚¹ã‚¯ç”¨ã«ã‚³ãƒ”ãƒ¼
        layout_copy = json.loads(json.dumps(layout_data))
        self._search_task = asyncio.ensure_future(self._search_and_place_assets(layout_copy))

    def _extract_layout_objects(self, layout_data) -> List[Dict[str, object]]:
        """ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é…ç½®å¯¾è±¡ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚"""
        if isinstance(layout_data, list):
            return [obj for obj in layout_data if isinstance(obj, dict)]

        if not isinstance(layout_data, dict):
            return []

        candidate_keys = [
            "area_objects_list",
            "objects",
            "items",
            "object_list",
            "layout_objects",
        ]
        for key in candidate_keys:
            value = layout_data.get(key)
            if isinstance(value, list):
                return [obj for obj in value if isinstance(obj, dict)]

        # ä¸€éƒ¨ã®JSONã§ã¯ layout_data["layout"]["objects"] ã®ã‚ˆã†ã«ãƒã‚¹ãƒˆã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹
        for value in layout_data.values():
            if isinstance(value, list):
                filtered = [obj for obj in value if isinstance(obj, dict)]
                if filtered:
                    return filtered
            elif isinstance(value, dict):
                nested = self._extract_layout_objects(value)
                if nested:
                    return nested

        return []

    
    def _place_debug_bboxes(
        self,
        layout_data: Dict[str, object],
        root_name: str = "DebugBBoxes",
        clear_existing: bool = True,
    ) -> None:
        """JSON?????????????????(BBox)??????"""
        if not layout_data:
            omni.log.warn("No layout data provided for debug bboxes.")
            return

        stage = omni.usd.get_context().get_stage()
        if stage is None:
            omni.log.error("No USD stage is available. Open or create a stage first.")
            return

        # ????????
        root_token = self._sanitize_identifier(root_name)
        root_path = f"/{root_token}"
        if clear_existing:
            existing = stage.GetPrimAtPath(root_path)
            if existing and existing.IsValid():
                stage.RemovePrim(root_path)

        UsdGeom.Xform.Define(stage, Sdf.Path(root_path))

        objects = self._extract_layout_objects(layout_data)
        if not objects:
            omni.log.warn("Layout data does not contain placeable objects for debug bboxes.")
            return

        # ?????????????
        category_colors = {
            "bed": Gf.Vec3f(0.7, 0.5, 0.9),
            "sofa": Gf.Vec3f(0.5, 0.7, 0.9),
            "table": Gf.Vec3f(0.8, 0.7, 0.4),
            "chair": Gf.Vec3f(0.6, 0.8, 0.6),
            "door": Gf.Vec3f(0.9, 0.6, 0.4),
            "window": Gf.Vec3f(0.4, 0.8, 0.9),
            "wall": Gf.Vec3f(0.7, 0.7, 0.7),
            "floor": Gf.Vec3f(0.5, 0.5, 0.5),
        }

        placed = 0
        skipped = 0
        for index, obj in enumerate(objects, start=1):
            name = str(obj.get("object_name", "") or obj.get("name", "") or f"item_{index}")
            category = str(obj.get("category", "") or "").strip().lower()

            length = self._extract_float(obj, "Length", 0.0)
            width = self._extract_float(obj, "Width", 0.0)
            height = self._extract_float(obj, "Height", 0.0)

            if length <= 0.0 or width <= 0.0 or height <= 0.0:
                omni.log.warn(
                    f"[DebugBBox] Skipping '{name}': invalid size (L={length}, W={width}, H={height})"
                )
                skipped += 1
                continue

            x = self._extract_float(obj, "X", 0.0)
            y = self._extract_float(obj, "Y", 0.0)
            z = self._extract_float(obj, "Z", 0.0)

            rotation = self._extract_optional_float_by_keys(
                obj,
                [
                    "rotationZ",
                    "RotationZ",
                    "rotation_z",
                    "Rotation_Z",
                    "rotationY",
                    "RotationY",
                    "rotation_y",
                    "Rotation_Y",
                    "rotation",
                    "Rotation",
                ],
            )
            rotation = rotation if rotation is not None else 0.0

            token = self._sanitize_identifier(name)
            prim_path = self._get_unique_child_path(stage, root_path, token)
            cube = UsdGeom.Cube.Define(stage, prim_path)
            cube.CreateSizeAttr(1.0)

            # ???
            color = category_colors.get(category, Gf.Vec3f(0.8, 0.8, 0.8))
            gprim = UsdGeom.Gprim(cube.GetPrim())
            gprim.CreateDisplayColorAttr([color])

            # ????????
            xformable = UsdGeom.Xformable(cube.GetPrim())
            translate_op = xformable.AddTranslateOp()
            rotate_op = xformable.AddRotateXYZOp()
            scale_op = xformable.AddScaleOp()

            translate_op.Set(Gf.Vec3f(x, y, z + height * 0.5))
            rotate_op.Set(Gf.Vec3f(0.0, 0.0, rotation))
            scale_op.Set(Gf.Vec3f(length, width, height))

            placed += 1

        omni.log.info(
            f"[DebugBBox] Placed {placed} boxes under '{root_path}'. Skipped={skipped}."
        )

    async def _search_and_place_assets(self, layout_data) -> None:
            """USD Search ã‚’ç”¨ã„ã¦ã‚¢ã‚»ãƒƒãƒˆæ¤œç´¢ã¨é…ç½®ã‚’è¡Œã†éžåŒæœŸã‚¿ã‚¹ã‚¯ã€‚"""
            placed = 0
            skipped = 0
    
            try:
                search_root = self._search_root_model.as_string if self._search_root_model else ""
                search_root = search_root.strip()
                if not search_root:
                    omni.log.error("Search root URL is empty. Set the 'Search Root URL' field before running placement.")
                    return
    
                if not search_root.startswith("omniverse://"):
                    omni.log.error(f"Search root must start with 'omniverse://'. Current value: {search_root}")
                    return
    
                normalized_root = search_root if search_root.endswith("/") else f"{search_root}/"
    
                stage = omni.usd.get_context().get_stage()
                if stage is None:
                    omni.log.error("No USD stage is available. Open or create a stage before placing assets.")
                    return
    
                area_name = layout_data.get("area_name") if isinstance(layout_data, dict) else None
                root_prim_path = self._get_or_create_root_prim(stage, area_name)
    
                objects = self._extract_layout_objects(layout_data)
                if not objects:
                    omni.log.warn(
                        "Layout data does not contain placeable objects. Expected keys such as "
                        "'area_objects_list', 'objects', or a top-level list of entries."
                    )
                    return
                size_mode = str(layout_data.get("size_mode") or "world").lower()
                if size_mode not in ("world", "local"):
                    size_mode = "world"
                for obj in objects:
                    if isinstance(obj, dict) and "size_mode" not in obj:
                        obj["size_mode"] = size_mode
                omni.log.info(f"Found {len(objects)} candidate objects for placement.")
    
                # ãƒ‰ã‚¢ã®ãƒªã‚¹ãƒˆã‚’åŽé›†ï¼ˆå£ã®åˆ‡ã‚ŠæŠœãç”¨ï¼‰
                door_objects = []
    
                for index, obj in enumerate(objects):
                    name = str(obj.get("object_name", "") or "").strip()
                    category = str(obj.get("category", "") or "").strip()
                    category_lower = category.lower()
                    search_prompt = str(obj.get("search_prompt", "") or "").strip()
    
                    if not (name or category or search_prompt):
                        omni.log.warn(f"Skipping object #{index + 1}: missing 'object_name', 'category', and 'search_prompt'.")
                        skipped += 1
                        continue
    
                    # åºŠã®å ´åˆã¯æ‰‹ç¶šãçš„ã«ç”Ÿæˆ
                    if name.lower() == "floor" or category_lower == "floor":
                        omni.log.info(f"[Procedural] Generating floor ({index + 1}/{len(objects)})")
                        floor_path = self._create_procedural_floor(stage, root_prim_path, obj)
                        if floor_path:
                            omni.log.info(f"Generated floor at {floor_path}")
                            placed += 1
                        else:
                            skipped += 1
                        continue
    
                    # ãƒ‰ã‚¢ã®å ´åˆã¯ãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆå¾Œã§å£ã®åˆ‡ã‚ŠæŠœãã«ä½¿ç”¨ï¼‰
                    if category_lower == "door" or "door" in name.lower():
                        door_objects.append(obj)
                        omni.log.info(f"[Door] Detected door '{name}' at X={obj.get('X', 0)}, Y={obj.get('Y', 0)}")
    
                    # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
                    search_query = self._build_search_query_from_object(obj)
    
                    omni.log.info(
                        f"[Search] Querying '{search_query}' "
                        f"(Original name: '{name}', category: '{category}') "
                        f"({index + 1}/{len(objects)})"
                    )
                    try:
                        # ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚»ãƒžãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢APIã‚’ä½¿ç”¨
                        # ç›´æŽ¥USDãƒ•ã‚¡ã‚¤ãƒ«ã®URLãŒè¿”ã•ã‚Œã‚‹ãŸã‚ã€å†å¸°çš„æ¤œç´¢ã¯ä¸è¦
                        asset_url = await self._semantic_search_asset(search_query, normalized_root)
                    except asyncio.CancelledError:
                        raise
                    except Exception as exc:
                        omni.log.error(f"Vector search failed for '{name}': {exc}")
                        skipped += 1
                        continue
    
                    if not asset_url:
                        omni.log.warn(f"No matching USD asset found for '{name}' (searched as '{search_query}').")
                        skipped += 1
                        continue
    
                    prim_path = await self._reference_asset(stage, root_prim_path, name, asset_url, obj)
                    if prim_path:
                        omni.log.info(f"Placed '{name}' at {prim_path}")
                        placed += 1
                    else:
                        skipped += 1
    
                    await asyncio.sleep(0)
    
                # ã™ã¹ã¦ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆé…ç½®ãŒå®Œäº†ã—ãŸå¾Œã€å£ã‚’ç”Ÿæˆ
                omni.log.info("=== Generating procedural walls ===")
                wall_paths: List[str] = []
                window_paths: List[str] = []
    
                rooms = layout_data.get("rooms") or []
                outer_polygon = layout_data.get("outer_polygon")
                room_polygon = layout_data.get("room_polygon")
                openings: List[Dict[str, object]] = []
    
                if rooms:
                    for room in rooms:
                        openings.extend(room.get("openings") or [])
                if isinstance(layout_data.get("openings"), list):
                    openings.extend(layout_data.get("openings") or [])
    
                openings = self._dedupe_openings(openings)
                interior_openings = [
                    op for op in openings if str(op.get("type", "") or "").lower() == "door"
                ]
                used_polygon_walls = False
    
                if rooms or outer_polygon or room_polygon:
                    try:
                        outer_edges: List[Dict[str, object]] = []
                        if outer_polygon:
                            outer_points = self._wall_generator._normalize_polygon_points(outer_polygon)
                            if len(outer_points) >= 3:
                                outer_edges = self._wall_generator._build_edges(outer_points)
                            if outer_edges:
                                wall_chunk, window_chunk = self._wall_generator.generate_walls_from_polygon(
                                    stage, root_prim_path, outer_polygon, openings
                                )
                                wall_paths.extend(wall_chunk)
                                window_paths.extend(window_chunk)
    
                        if rooms:
                            room_edges: List[Dict[str, object]] = []
                            for room in rooms:
                                polygon = room.get("room_polygon")
                                if not polygon:
                                    continue
                                room_points = self._wall_generator._normalize_polygon_points(polygon)
                                if len(room_points) < 3:
                                    continue
                                room_edges.extend(self._wall_generator._build_edges(room_points))
    
                            if room_edges:
                                merged_edges = self._merge_axis_aligned_edges(room_edges)
                                if outer_edges:
                                    merged_edges = self._filter_edges_against_outer(
                                        merged_edges, outer_edges
                                    )
                                if merged_edges:
                                    wall_chunk, window_chunk = self._wall_generator.generate_walls_from_edges(
                                        stage, root_prim_path, merged_edges, interior_openings
                                    )
                                    wall_paths.extend(wall_chunk)
                                    window_paths.extend(window_chunk)
    
                        elif room_polygon and not outer_polygon:
                            wall_chunk, window_chunk = self._wall_generator.generate_walls_from_polygon(
                                stage, root_prim_path, room_polygon, openings
                            )
                            wall_paths.extend(wall_chunk)
                            window_paths.extend(window_chunk)
    
                        used_polygon_walls = bool(wall_paths or window_paths)
                    except Exception as exc:
                        omni.log.warn(
                            f"Polygon wall generation failed, falling back to rectangular walls: {exc}"
                        )
                        used_polygon_walls = False

                if not used_polygon_walls:
                    area_size_x = layout_data.get("area_size_X")
                    area_size_y = layout_data.get("area_size_Y")
                    origin_mode = "center"

                    if isinstance(area_size_x, (int, float)) and isinstance(area_size_y, (int, float)):
                        # Heuristic: if all object coords are inside [0, size], treat origin as bottom-left.
                        try:
                            xs = [float(obj.get("X")) for obj in objects if obj.get("X") is not None]
                            ys = [float(obj.get("Y")) for obj in objects if obj.get("Y") is not None]
                            if xs and ys:
                                min_x, max_x = min(xs), max(xs)
                                min_y, max_y = min(ys), max(ys)
                                tol = 1e-3
                                if (
                                    min_x >= -tol
                                    and min_y >= -tol
                                    and max_x <= float(area_size_x) + tol
                                    and max_y <= float(area_size_y) + tol
                                ):
                                    origin_mode = "bottom_left"
                        except Exception:
                            origin_mode = "center"

                        wall_paths = self._create_procedural_walls(
                            stage,
                            root_prim_path,
                            float(area_size_x),
                            float(area_size_y),
                            door_objects,
                            origin_mode,
                        )
                    else:
                        omni.log.warn(
                            "Cannot generate walls: area_size_X or area_size_Y not found in layout data. "
                            f"area_size_X={area_size_x}, area_size_Y={area_size_y}"
                        )

                placed += len(wall_paths) + len(window_paths)
                omni.log.info(
                    f"Generated {len(wall_paths)} wall segments and {len(window_paths)} window panes"
                )
                omni.log.info(f"USD Search placement finished. Placed={placed}, Skipped={skipped}")
            except asyncio.CancelledError:
                omni.log.warn("USD Search placement cancelled.")
                raise
            except Exception as exc:
                omni.log.error(f"Unexpected error during USD Search placement: {exc}")
            finally:
                self._search_task = None

    async def _place_debug_bboxes(self, layout_data) -> None:
        """Place cubes representing JSON bounding boxes (debug mode)."""
        placed = 0
        skipped = 0

        try:
            stage = omni.usd.get_context().get_stage()
            if stage is None:
                omni.log.error("No USD stage is available. Open or create a stage before placing debug boxes.")
                return

            area_name = layout_data.get("area_name") if isinstance(layout_data, dict) else None
            root_prim_path = self._get_or_create_root_prim(stage, area_name)

            objects = self._extract_layout_objects(layout_data)
            if not objects:
                omni.log.warn("Layout data does not contain placeable objects for debug bbox placement.")
                return

            size_mode = str(layout_data.get("size_mode") or "world").lower()
            if size_mode not in ("world", "local"):
                size_mode = "world"
            for obj in objects:
                if isinstance(obj, dict) and "size_mode" not in obj:
                    obj["size_mode"] = size_mode

            omni.log.info(f"[DebugBBox] Placing {len(objects)} bounding boxes...")

            for index, obj in enumerate(objects):
                name = str(obj.get("object_name", "") or "").strip()
                if not name:
                    name = f"Object_{index + 1}"

                prim_path = self._create_debug_bbox(stage, root_prim_path, name, obj)
                if prim_path:
                    placed += 1
                else:
                    skipped += 1

                await asyncio.sleep(0)

            omni.log.info(f"[DebugBBox] Done. Placed={placed}, Skipped={skipped}")
        except asyncio.CancelledError:
            omni.log.warn("Debug bbox placement cancelled.")
            raise
        except Exception as exc:
            omni.log.error(f"Debug bbox placement failed: {exc}")
        finally:
            self._search_task = None

    def _create_debug_bbox(
        self,
        stage,
        root_prim_path: str,
        object_name: str,
        object_data: Dict[str, object],
    ) -> Optional[str]:
        """Create a cube that represents the JSON bounding box (world-aligned)."""
        try:
            length = self._extract_float(object_data, "Length", 0.0)
            width = self._extract_float(object_data, "Width", 0.0)
            height = self._extract_float(object_data, "Height", 0.0)

            if length <= 0.0 or width <= 0.0 or height <= 0.0:
                omni.log.warn(
                    f"[DebugBBox] Invalid size for '{object_name}': L={length}, W={width}, H={height}"
                )
                return None

            x = self._extract_float(object_data, "X", 0.0)
            y = self._extract_float(object_data, "Y", 0.0)
            rotation = (
                self._extract_optional_float_by_keys(
                    object_data,
                    [
                        "rotationZ",
                        "RotationZ",
                        "rotation_z",
                        "Rotation_Z",
                        "rotation",
                        "Rotation",
                    ],
                )
                or 0.0
            )
            size_mode = str(object_data.get("size_mode") or "world").lower()
            if size_mode not in ("world", "local"):
                size_mode = "world"
            front_rotation_source = 0.0 if size_mode == "local" else rotation

            front_thickness = max(0.02, min(0.1, width * 0.1))

            # Direction mapping: rotationZ=0 => +Y, 90 => +X, 180 => -Y, 270 => -X
            theta = math.radians(front_rotation_source)
            dir_x = math.sin(theta)
            dir_y = math.cos(theta)

            hx = length / 2.0
            hy = width / 2.0
            denom = 0.0
            if hx > 1e-6:
                denom = max(denom, abs(dir_x) / hx)
            if hy > 1e-6:
                denom = max(denom, abs(dir_y) / hy)
            if denom <= 0.0:
                boundary_x = 0.0
                boundary_y = 0.0
            else:
                scale = 1.0 / denom
                boundary_x = dir_x * scale
                boundary_y = dir_y * scale

            front_offset_x = boundary_x + dir_x * (front_thickness / 2.0)
            front_offset_y = boundary_y + dir_y * (front_thickness / 2.0)

            # Front plane rotation so that its local +Y aligns to the direction vector
            if abs(dir_x) > 1e-6 or abs(dir_y) > 1e-6:
                front_rotation = math.degrees(math.atan2(-dir_x, dir_y))
            else:
                front_rotation = 0.0

            # Span of the front plane along the tangent direction
            plane_span = length if abs(dir_y) >= abs(dir_x) else width

            child_token = self._sanitize_identifier(
                f"BBox_{object_name}" if object_name else "BBox"
            )
            root_path = self._get_unique_child_path(stage, root_prim_path, child_token)
            root_xform = UsdGeom.Xform.Define(stage, Sdf.Path(root_path))

            root_ops = UsdGeom.Xformable(root_xform)
            # Interpret JSON coordinates in world space
            try:
                root_ops.SetResetXformStack(True)
            except Exception:
                pass
            root_ops.AddTranslateOp().Set(Gf.Vec3d(x, y, 0.0))
            if size_mode == "local":
                root_ops.AddRotateZOp().Set(rotation)

            body_path = f"{root_path}/Body"
            body = UsdGeom.Cube.Define(stage, Sdf.Path(body_path))
            body.GetSizeAttr().Set(1.0)
            try:
                body.CreateDisplayColorAttr().Set([(0.2, 0.7, 1.0)])
                body.CreateDisplayOpacityAttr().Set([0.2])
            except Exception:
                pass

            body_xform = UsdGeom.Xformable(body)
            body_xform.AddTranslateOp().Set(Gf.Vec3d(0.0, 0.0, height / 2.0))
            body_xform.AddScaleOp().Set(Gf.Vec3f(length, width, height))

            front_path = f"{root_path}/Front"
            front = UsdGeom.Cube.Define(stage, Sdf.Path(front_path))
            front.GetSizeAttr().Set(1.0)
            try:
                front.CreateDisplayColorAttr().Set([(1.0, 0.2, 0.2)])
                front.CreateDisplayOpacityAttr().Set([0.6])
            except Exception:
                pass

            front_xform = UsdGeom.Xformable(front)
            front_xform.AddTranslateOp().Set(Gf.Vec3d(front_offset_x, front_offset_y, height / 2.0))
            front_xform.AddRotateZOp().Set(front_rotation)
            front_xform.AddScaleOp().Set(Gf.Vec3f(plane_span, front_thickness, height))

            return root_path
        except Exception as exc:
            omni.log.error(f"[DebugBBox] Failed to create bbox for '{object_name}': {exc}")
            return None

    async def _blacklist_and_replace_selected_asset(self, prim_path: str) -> None:
        await self._replace_selected_asset(prim_path, add_to_blacklist=True)

    async def _replace_selected_asset(self, prim_path: str, add_to_blacklist: bool = False) -> None:
        stage = omni.usd.get_context().get_stage()
        if stage is None:
            omni.log.error("No USD stage is available. Open or create a stage before replacing assets.")
            return

        prim = stage.GetPrimAtPath(prim_path)
        if not prim or not prim.IsValid():
            omni.log.warn(f"Invalid prim selected: {prim_path}")
            return

        ref_prim = self._get_reference_prim(prim)
        asset_url = self._get_asset_url_from_prim(ref_prim)
        if not asset_url:
            omni.log.warn(f"Selected prim has no asset reference: {prim_path}")
            return

        identity_key = await self._get_asset_identity_key(asset_url)
        if add_to_blacklist:
            added_url = self._add_asset_to_blacklist(asset_url)
            added_key = self._add_asset_key_to_blacklist(identity_key)
            if added_url or added_key:
                self._save_settings_to_json()
                self._update_blacklist_label()
                self._rebuild_blacklist_list()
                omni.log.info(
                    f"Blacklisted asset: {self._normalize_asset_url(asset_url)} (key={identity_key})"
                )
            else:
                omni.log.info(
                    f"Asset already blacklisted: {self._normalize_asset_url(asset_url)} (key={identity_key})"
                )

        data = self._build_data_from_prim_metadata(ref_prim)
        if not data:
            omni.log.warn("Selected prim has no placement metadata. Cannot replace asset in-place.")
            return

        meta = self._get_search_metadata_from_prim(ref_prim)
        search_query = meta.get("search_query") or self._build_search_query_from_object(meta)
        if not search_query:
            omni.log.warn("Unable to determine search query for selected asset.")
            return

        search_root = self._search_root_model.as_string if self._search_root_model else ""
        search_root = search_root.strip()
        if not search_root:
            omni.log.error("Search root URL is empty. Set the 'Search Root URL' field before replacing assets.")
            return
        if not search_root.startswith("omniverse://"):
            omni.log.error(f"Search root must start with 'omniverse://'. Current value: {search_root}")
            return
        normalized_root = search_root if search_root.endswith("/") else f"{search_root}/"

        replacement_url = await self._semantic_search_asset(
            search_query,
            normalized_root,
            exclude_urls={asset_url},
            exclude_keys={identity_key} if identity_key else None,
        )
        if not replacement_url:
            omni.log.warn(f"No replacement asset found for query '{search_query}'.")
            return

        omni.log.info(
            f"Replacing asset '{asset_url}' with '{replacement_url}' for prim '{ref_prim.GetPath()}'"
        )
        references = ref_prim.GetReferences()
        references.ClearReferences()
        references.AddReference(replacement_url)
        await self._apply_transform(ref_prim, data, replacement_url)


    async def _replace_selected_asset_with_url(
        self,
        prim_path: str,
        replacement_url: str,
        add_to_blacklist: bool = False,
    ) -> None:
        stage = omni.usd.get_context().get_stage()
        if stage is None:
            omni.log.error("No USD stage is available. Open or create a stage before replacing assets.")
            return

        if not replacement_url:
            omni.log.warn("Replacement URL is empty. Abort replace.")
            return

        prim = stage.GetPrimAtPath(prim_path)
        if not prim or not prim.IsValid():
            omni.log.warn(f"Invalid prim selected: {prim_path}")
            return

        ref_prim = self._get_reference_prim(prim)
        asset_url = self._get_asset_url_from_prim(ref_prim)
        if not asset_url:
            omni.log.warn(f"Selected prim has no asset reference: {prim_path}")
            return

        replacement_url = str(replacement_url).strip()
        if not replacement_url:
            omni.log.warn("Replacement URL is empty after normalization.")
            return

        history_key = str(ref_prim.GetPath())
        identity_key = await self._get_asset_identity_key(asset_url)
        self._append_replacement_history(history_key, asset_url, identity_key)
        if add_to_blacklist:
            added_url = self._add_asset_to_blacklist(asset_url)
            added_key = self._add_asset_key_to_blacklist(identity_key)
            if added_url or added_key:
                self._save_settings_to_json()
                self._update_blacklist_label()
                self._rebuild_blacklist_list()
                omni.log.info(
                    f"Blacklisted asset: {self._normalize_asset_url(asset_url)} (key={identity_key})"
                )
            else:
                omni.log.info(
                    f"Asset already blacklisted: {self._normalize_asset_url(asset_url)} (key={identity_key})"
                )

        data = self._build_data_from_prim_metadata(ref_prim)
        if not data:
            omni.log.warn("Selected prim has no placement metadata. Cannot replace asset in-place.")
            return

        if self._normalize_asset_url(replacement_url) == self._normalize_asset_url(asset_url):
            omni.log.info("Replacement URL matches current asset. Skipping replace.")
            return

        omni.log.info(
            f"Replacing asset '{asset_url}' with '{replacement_url}' for prim '{ref_prim.GetPath()}'"
        )
        references = ref_prim.GetReferences()
        references.ClearReferences()
        references.AddReference(replacement_url)
        await self._apply_transform(ref_prim, data, replacement_url)
        replacement_key = await self._get_asset_identity_key(replacement_url)
        self._append_replacement_history(history_key, replacement_url, replacement_key)

    def _get_or_create_root_prim(self, stage, area_name: Optional[str]) -> str:
        """é…ç½®å…ˆã®ãƒ«ãƒ¼ãƒˆXformã‚’å–å¾—ã¾ãŸã¯ä½œæˆã™ã‚‹ã€‚"""
        token = self._sanitize_identifier(area_name or "AssetPlacer")
        root_path = f"/{token}"
        if not stage.GetPrimAtPath(root_path):
            UsdGeom.Xform.Define(stage, Sdf.Path(root_path))
        return root_path

    async def _reference_asset(
        self,
        stage,
        root_prim_path: str,
        object_name: str,
        asset_url: str,
        object_data: Dict[str, object],
    ) -> Optional[str]:
        """ã‚¢ã‚»ãƒƒãƒˆã‚’å‚ç…§ã¨ã—ã¦é…ç½®ã—ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ ã‚’é©ç”¨ã™ã‚‹ã€‚"""
        try:
            child_token = self._sanitize_identifier(object_name or "Asset")
            prim_path = self._get_unique_child_path(stage, root_prim_path, child_token)
            prim = stage.DefinePrim(prim_path, "Xform")
            references = prim.GetReferences()
            references.ClearReferences()
            references.AddReference(asset_url)
            await self._apply_transform(prim, object_data, asset_url)
            return prim_path
        except Exception as exc:
            omni.log.error(f"Failed to reference asset '{asset_url}' for '{object_name}': {exc}")
            return None

    def _get_unique_child_path(self, stage, parent_path: str, child_token: str) -> str:
        """åŒåPrimãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯é€£ç•ªã‚’ä»˜ä¸Žã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ‘ã‚¹ã‚’è¿”ã™ã€‚"""
        parent_sdf = Sdf.Path(parent_path)
        candidate = parent_sdf.AppendChild(child_token)
        suffix = 1
        while stage.GetPrimAtPath(candidate):
            candidate = parent_sdf.AppendChild(f"{child_token}_{suffix}")
            suffix += 1
        return str(candidate)

    async def _apply_transform(self, prim, data: Dict[str, object], asset_url: Optional[str] = None) -> None:
        """
        ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæƒ…å ±ã‹ã‚‰å–å¾—ã—ãŸãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ ã‚’Primã«è¨­å®šã™ã‚‹ã€‚

        JSONã®å¯¸æ³•ï¼ˆãƒ¡ãƒ¼ãƒˆãƒ«å˜ä½ï¼‰ã‚’çµ¶å¯¾çš„ãªæœ€çµ‚å¯¸æ³•ã¨ã—ã¦æ‰±ã„ã€
        ã‚¢ã‚»ãƒƒãƒˆã®å…ƒã®ã‚µã‚¤ã‚ºã«åŸºã¥ã„ã¦ã‚¹ã‚±ãƒ¼ãƒ«æ¯”ã‚’è¨ˆç®—ã—ã¦é©ç”¨ã™ã‚‹ã€‚
        """
        # ã‚¢ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ã‚’å¾…æ©Ÿï¼ˆæœ€å¤§10ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
        # å‚ç…§ã‚¢ã‚»ãƒƒãƒˆã®ã‚¸ã‚ªãƒ¡ãƒˆãƒªãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã¾ã§ã€BBoxã¯æ­£ã—ãè¨ˆç®—ã•ã‚Œãªã„
        max_retries = 10
        time_code = Usd.TimeCode.Default()

        # å‚ç…§å…ˆã®upAxisã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰ã€‚asset_urlãŒæœªæŒ‡å®šã®å ´åˆã¯å‚ç…§ã‚’èª¿æŸ»ã€‚
        if not asset_url:
            asset_url = self._get_asset_url_from_prim(prim)

        up_axis = self._get_asset_up_axis(asset_url) if asset_url else "Z"
        omni.log.info(f"[UpAxis] Using upAxis='{up_axis}' for prim '{prim.GetPath()}', asset='{asset_url}'")

        for attempt in range(max_retries):
            # 1ãƒ•ãƒ¬ãƒ¼ãƒ å¾…æ©Ÿã—ã¦USDãŒã‚¢ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ™‚é–“ã‚’ä¸Žãˆã‚‹
            await omni.kit.app.get_app().next_update_async()

            # BBoxãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
            bbox_cache = UsdGeom.BBoxCache(time_code, ["default"])
            bbox = bbox_cache.ComputeWorldBound(prim)
            bbox_range = bbox.ComputeAlignedRange()
            size_vec = bbox_range.GetMax() - bbox_range.GetMin()

            # ã‚µã‚¤ã‚ºãŒéžã‚¼ãƒ­ï¼ˆã‚¸ã‚ªãƒ¡ãƒˆãƒªãŒãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸï¼‰ãªã‚‰å¾…æ©Ÿå®Œäº†
            if size_vec[0] > 1e-6 or size_vec[1] > 1e-6 or size_vec[2] > 1e-6:
                omni.log.info(f"Asset geometry loaded after {attempt + 1} frame(s)")
                break
        else:
            # æœ€å¤§è©¦è¡Œå›žæ•°ã«é”ã—ã¦ã‚‚ãƒ­ãƒ¼ãƒ‰ã•ã‚Œãªã„å ´åˆã¯è­¦å‘Š
            omni.log.warn(f"Asset geometry may not be fully loaded after {max_retries} frames")

        xformable = UsdGeom.Xformable(prim)

        # ã‚¹ãƒ†ãƒƒãƒ—1: æ—¢å­˜ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ ã®ã‚¯ãƒªã‚¢
        try:
            for op in xformable.GetOrderedXformOps():
                prim.RemoveProperty(op.GetAttr().GetName())
            if hasattr(xformable, "ClearXformOpOrder"):
                xformable.ClearXformOpOrder()
            else:
                xformable.SetXformOpOrder([])
        except Exception as exc:  # pragma: no cover - defensive cleanup
            omni.log.warn(f"Failed clearing existing xform ops on '{prim.GetPath()}': {exc}")

        # ã‚¹ãƒ†ãƒƒãƒ—2: ã€Œå…ƒã®ã‚µã‚¤ã‚ºã€ã®è¨ˆç®—
        # å¾…æ©Ÿå‡¦ç†ã§æ—¢ã«è¨ˆç®—ã—ãŸBBoxã‚’å†åˆ©ç”¨
        # ï¼ˆå¿µã®ãŸã‚å†è¨ˆç®—ã—ã¦æœ€æ–°ã®çŠ¶æ…‹ã‚’å–å¾—ï¼‰
        bbox_cache = UsdGeom.BBoxCache(time_code, ["default"])
        bbox = bbox_cache.ComputeWorldBound(prim)
        bbox_range = bbox.ComputeAlignedRange()

        # å…ƒã®ã‚µã‚¤ã‚ºï¼ˆãƒ¡ãƒ¼ãƒˆãƒ«å˜ä½ï¼‰ã‚’å–å¾—
        original_size_vec = bbox_range.GetMax() - bbox_range.GetMin()
        original_size_x = original_size_vec[0]
        original_size_y = original_size_vec[1]
        original_size_z = original_size_vec[2]

        omni.log.info(
            f"Original asset size (m): X={original_size_x:.4f}, Y={original_size_y:.4f}, Z={original_size_z:.4f}"
        )

        # ã‚¹ãƒ†ãƒƒãƒ—3: å›žè»¢ã‚’å…ˆã«å–å¾—ï¼ˆZè»¸å‘¨ã‚Šï¼‰
        rotation = self._extract_optional_float_by_keys(
            data,
            [
                "rotationZ",
                "RotationZ",
                "rotation_z",
                "Rotation_Z",
                "rotationY",
                "RotationY",
                "rotation_y",
                "Rotation_Y",
                "rotation",
                "Rotation",
            ],
        )

        base_rotation = rotation if rotation is not None else 0.0
        rotation_offset = self._get_asset_rotation_offset(asset_url)
        effective_rotation = (base_rotation + rotation_offset) % 360.0
        if rotation_offset:
            omni.log.info(
                f"[Transform] rotation base={base_rotation} deg, offset={rotation_offset} deg, "
                f"effective={effective_rotation} deg"
            )
        size_mode = str(data.get("size_mode") or "world").lower()
        if size_mode not in ("world", "local"):
            size_mode = "world"

        # ã‚¹ãƒ†ãƒƒãƒ—4: ã€Œç›®æ¨™ã®ã‚µã‚¤ã‚ºã€ã®èª­ã¿å–ã‚Šï¼ˆZ-Upåº§æ¨™ç³»ï¼‰
        # JSONã‹ã‚‰Length, Height, Widthï¼ˆãƒ¡ãƒ¼ãƒˆãƒ«å˜ä½ï¼‰ã‚’èª­ã¿å–ã‚Š
        json_length = self._extract_float(data, "Length", 0.0)
        json_height = self._extract_float(data, "Height", 0.0)
        json_width = self._extract_float(data, "Width", 0.0)
        object_name = str(data.get("object_name", "") or "")
        category = str(data.get("category", "") or "")
        category_lower = category.lower()
        is_door = category_lower == "door" or "door" in object_name.lower()

        if size_mode == "local":
            # Local mode: Length/Width/Height are object-local axes (X=Right, Y=Forward, Z=Up).
            target_size_x = json_length
            target_size_y = json_width
            target_size_z = json_height

            # XformOps order: Translate(world) -> RotateZ(world) -> Scale(world) -> RotateZ(offset) -> RotateX(up)
            translate_op = xformable.AddTranslateOp(opSuffix="world")
            rotate_world_op = xformable.AddRotateZOp(opSuffix="world")
            scale_op = xformable.AddScaleOp(opSuffix="world")
            rotate_offset_op = xformable.AddRotateZOp(opSuffix="offset")
            rotate_up_op = None
            if up_axis == "Y":
                rotate_up_op = xformable.AddRotateXOp(opSuffix="up")

            translate_op.Set(Gf.Vec3d(0.0, 0.0, 0.0))
            rotate_world_op.Set(0.0)
            scale_op.Set(Gf.Vec3f(1.0, 1.0, 1.0))
            rotate_offset_op.Set(rotation_offset)
            if rotate_up_op:
                rotate_up_op.Set(90.0)

            # Measure normalized size after offset + upAxis (world rotation disabled).
            bbox_cache_local = UsdGeom.BBoxCache(time_code, ["default"])
            bbox_local = bbox_cache_local.ComputeWorldBound(prim)
            bbox_range_local = bbox_local.ComputeAlignedRange()
            normalized_vec = bbox_range_local.GetMax() - bbox_range_local.GetMin()
            normalized_size_x = normalized_vec[0]
            normalized_size_y = normalized_vec[1]
            normalized_size_z = normalized_vec[2]

            def safe_div(num: float, denom: float) -> float:
                if abs(denom) > 1e-6 and num > 0:
                    return num / denom
                return 1.0

            scale_x = safe_div(target_size_x, normalized_size_x)
            scale_y = safe_div(target_size_y, normalized_size_y)
            scale_z = safe_div(target_size_z, normalized_size_z)
            final_scale = Gf.Vec3f(scale_x, scale_y, scale_z)
            scale_op.Set(final_scale)

            # Apply world rotation after scale.
            rotate_world_op.Set(base_rotation)

            # Compute bbox after scale + world rotation to place in world.
            bbox_cache2 = UsdGeom.BBoxCache(time_code, ["default"])
            bbox2 = bbox_cache2.ComputeWorldBound(prim)
            bbox_range2 = bbox2.ComputeAlignedRange()
            min_after = bbox_range2.GetMin()
            max_after = bbox_range2.GetMax()
            min_z_after_rot_scale = min_after[2]
            center_x_after = (min_after[0] + max_after[0]) * 0.5
            center_y_after = (min_after[1] + max_after[1]) * 0.5

            x = self._extract_float(data, "X", 0.0)
            y = self._extract_float(data, "Y", 0.0)
            x -= center_x_after
            y -= center_y_after

            search_prompt = str(data.get("search_prompt", "") or "")
            search_query = self._build_search_query_from_object({
                "object_name": object_name,
                "category": category,
                "search_prompt": search_prompt,
            })
            self._store_placement_metadata(
                prim,
                asset_url,
                base_rotation,
                json_length,
                json_width,
                json_height,
                x,
                y,
                object_name=object_name,
                category=category,
                search_prompt=search_prompt,
                search_query=search_query,
            )

            translate_z = -min_z_after_rot_scale
            translate_op.Set(Gf.Vec3d(x, y, translate_z))

            omni.log.info(
                "[LocalMode] "
                f"object='{object_name}', category='{category}', size_mode={size_mode}, "
                f"target=({target_size_x:.4f}, {target_size_y:.4f}, {target_size_z:.4f}), "
                f"normalized=({normalized_size_x:.4f}, {normalized_size_y:.4f}, {normalized_size_z:.4f}), "
                f"scale=({scale_x:.4f}, {scale_y:.4f}, {scale_z:.4f}), "
                f"rotationZ={base_rotation}, offset={rotation_offset}, up_axis={up_axis}, "
                f"translate_z={translate_z:.4f}"
            )

            return


        # Z-Up axis mapping
        # 0/180 deg: Length->X, Width->Y
        # 90/270 deg: swap Length/Width (Width->X, Length->Y)
        if is_door:
            # Door sizing: map width/thickness to the asset's horizontal axes (no rotation-based swap).
            door_width = max(json_length, json_width)
            door_thickness = min(json_length, json_width)
            if door_width <= 0.0 and json_length > 0.0:
                door_width = json_length
            if door_thickness <= 0.0 and json_width > 0.0:
                door_thickness = json_width

            if up_axis == "Y":
                # Horizontal axes are X and Z when Y is up.
                if original_size_x >= original_size_z:
                    target_size_x = door_width
                    target_size_y = door_thickness  # target_y maps to local Z in _compute_scale_with_up_axis
                else:
                    target_size_x = door_thickness
                    target_size_y = door_width
            else:
                # Z-up: horizontal axes are X and Y.
                if original_size_x >= original_size_y:
                    target_size_x = door_width
                    target_size_y = door_thickness
                else:
                    target_size_x = door_thickness
                    target_size_y = door_width

            omni.log.info(
                f"[Transform] Door sizing: width={door_width:.4f}, thickness={door_thickness:.4f}, "
                f"mapped_to=({target_size_x:.4f}, {target_size_y:.4f})"
            )
        else:
            rotation_for_scale = effective_rotation
            if abs(rotation_for_scale - 90) < 45 or abs(rotation_for_scale - 270) < 45:
                target_size_x = json_width   # Width -> X
                target_size_y = json_length  # Length -> Y
                omni.log.info(
                    "Rotation "
                    f"{rotation_for_scale} deg detected: swapping Length/Width mapping (Width->X, Length->Y)"
                )
            else:
                target_size_x = json_length  # Length -> X
                target_size_y = json_width   # Width -> Y


        target_size_z = json_height  # Height â†’ Zè»¸ï¼ˆå¸¸ã«åŒã˜ï¼‰

        # ã‚¹ãƒ†ãƒƒãƒ—5: ã€Œã‚¹ã‚±ãƒ¼ãƒ«æ¯”ã€ã®è¨ˆç®—ï¼ˆupAxisã«å¿œã˜ã¦Y/Zã‚’å…¥ã‚Œæ›¿ãˆï¼‰
        scale_x, scale_y, scale_z = self._compute_scale_with_up_axis(
            up_axis,
            (original_size_x, original_size_y, original_size_z),
            (target_size_x, target_size_y, target_size_z),
        )
        final_scale = Gf.Vec3f(scale_x, scale_y, scale_z)

        omni.log.info(f"[Transform] up_axis={up_axis}, scale=({scale_x:.4f}, {scale_y:.4f}, {scale_z:.4f})")
        omni.log.info(
            f"[Transform] Target size (m): X={target_size_x:.4f}, Y={target_size_y:.4f}, Z={target_size_z:.4f}"
        )

        # ã‚¹ãƒ†ãƒƒãƒ—6: XformOpsã®è¿½åŠ ï¼ˆé©ç”¨é †: Scale -> RotateX(å¿…è¦æ™‚) -> RotateZ -> Translateï¼‰
        # è¿½åŠ é †ã¯é€†ï¼ˆTranslate -> RotateZ -> RotateX -> Scaleï¼‰
        translate_op = xformable.AddTranslateOp()
        translate_op.Set(Gf.Vec3d(0.0, 0.0, 0.0))

        rotate_z_op = xformable.AddRotateZOp()
        rotate_z_op.Set(effective_rotation)

        if up_axis == "Y":
            rotate_x_op = xformable.AddRotateXOp()
            rotate_x_op.Set(90.0)

        scale_op = xformable.AddScaleOp()
        scale_op.Set(final_scale)

        # ã‚¹ãƒ†ãƒƒãƒ—7: å›žè»¢+ã‚¹ã‚±ãƒ¼ãƒ«å¾Œã®BBoxã‚’å–å¾—ã—ã€åºŠåˆã‚ã›ã®ãŸã‚ã®Translate Zã‚’æ±ºå®š
        bbox_cache2 = UsdGeom.BBoxCache(time_code, ["default"])
        bbox2 = bbox_cache2.ComputeWorldBound(prim)
        bbox_range2 = bbox2.ComputeAlignedRange()
        min_after = bbox_range2.GetMin()
        max_after = bbox_range2.GetMax()
        min_z_after_rot_scale = min_after[2]
        center_x_after = (min_after[0] + max_after[0]) * 0.5
        center_y_after = (min_after[1] + max_after[1]) * 0.5

        # ã‚¹ãƒ†ãƒƒãƒ—8: JSONåº§æ¨™ã‚’é©ç”¨ï¼ˆZã¯åºŠåˆã‚ã›ã§ä¸Šæ›¸ãï¼‰
        x = self._extract_float(data, "X", 0.0)
        y = self._extract_float(data, "Y", 0.0)
        # object_name/category/is_door are extracted above
        if is_door and (abs(center_x_after) > 1e-4 or abs(center_y_after) > 1e-4):
            omni.log.info(
                f"[Transform] Door center align: center_offset=({center_x_after:.4f}, {center_y_after:.4f})"
            )
            x -= center_x_after
            y -= center_y_after
        search_prompt = str(data.get("search_prompt", "") or "")
        search_query = self._build_search_query_from_object({
            "object_name": object_name,
            "category": category,
            "search_prompt": search_prompt,
        })

        self._store_placement_metadata(
            prim,
            asset_url,
            base_rotation,
            json_length,
            json_width,
            json_height,
            x,
            y,
            object_name=object_name,
            category=category,
            search_prompt=search_prompt,
            search_query=search_query,
        )
        translate_z = -min_z_after_rot_scale
        translate_op.Set(Gf.Vec3d(x, y, translate_z))

        omni.log.info(
            f"[Transform] bbox_min_after_rot_scale={min_z_after_rot_scale:.4f}, "
            f"final_translate=({x:.4f}, {y:.4f}, {translate_z:.4f})"
        )

    def _select_asset_url(
        self,
        response,
        host: str,
        object_name: str,
    ) -> Optional[str]:
        """USD Searchã®çµæžœã‹ã‚‰æœ€é©ãªã‚¢ã‚»ãƒƒãƒˆURLã‚’é¸æŠžã™ã‚‹ã€‚"""
        paths = getattr(response, "paths", None)
        if not paths:
            omni.log.warn(f"No paths found in search response for '{object_name}'")
            return None

        normalized_target = self._normalize_name(object_name)
        candidates: List[str] = []
        for path in paths:
            uri = getattr(path, "uri", None)
            if not uri:
                continue
            candidate = f"omniverse://{host}{uri}"
            if not candidate.lower().endswith(".usd"):
                continue
            candidates.append(candidate)

        if not candidates:
            omni.log.warn(f"No USD candidates found for '{object_name}' (searched {len(paths)} paths)")
            return None

        omni.log.info(f"Found {len(candidates)} USD candidates for '{object_name}':")
        for candidate in candidates:
            omni.log.info(f"  - {candidate}")

        # Step 1: å®Œå…¨ä¸€è‡´ã‚’è©¦ã¿ã‚‹
        omni.log.info(f"Trying exact match for normalized target: '{normalized_target}'")
        for candidate in candidates:
            base = candidate.split("/")[-1]
            normalized_base = self._normalize_name(base.split(".")[0])
            omni.log.info(f"  Comparing '{normalized_base}' == '{normalized_target}'")
            if normalized_base == normalized_target:
                omni.log.info(f"  -> Exact match found: {candidate}")
                return candidate

        # Step 2: éƒ¨åˆ†ä¸€è‡´ã‚’è©¦ã¿ã‚‹ï¼ˆã‚¯ã‚¨ãƒªã®å„å˜èªžãŒãƒ•ã‚¡ã‚¤ãƒ«åã¨ä¸€è‡´ã™ã‚‹ã‹ï¼‰
        omni.log.info(f"Trying partial match with query words: {object_name.lower().split()}")
        query_words = object_name.lower().split()
        for candidate in candidates:
            base = candidate.split("/")[-1]
            normalized_base = self._normalize_name(base.split(".")[0])
            for word in query_words:
                normalized_word = self._normalize_name(word)
                omni.log.info(f"  Checking if '{normalized_word}' matches '{normalized_base}'")
                # å˜èªžãŒãƒ•ã‚¡ã‚¤ãƒ«åã¨å®Œå…¨ä¸€è‡´ã€ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã¾ã‚Œã‚‹å ´åˆ
                if normalized_word == normalized_base or normalized_word in normalized_base:
                    omni.log.info(f"  -> Partial match found: {candidate}")
                    return candidate

        # Step 3: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæœ€åˆã®å€™è£œã‚’è¿”ã™ï¼‰
        omni.log.info(f"No match found, using first candidate: {candidates[0]}")
        return candidates[0]

    @staticmethod
    def _sanitize_identifier(name: str) -> str:
        token = Tf.MakeValidIdentifier(name) if name else ""
        return token or "Item"

    @staticmethod
    def _normalize_name(value: str) -> str:
        return "".join(ch for ch in value.lower() if ch.isalnum())

    def _extract_float(self, data: Dict[str, object], key: str, default: float = 0.0) -> float:
        value = self._get_value_case_insensitive(data, key)
        if value is None:
            return default
        try:
            return float(value)
        except (TypeError, ValueError):
            omni.log.warn(f"Invalid numeric value '{value}' for key '{key}'. Using default {default}.")
            return default

    def _extract_optional_float(self, data: Dict[str, object], key: str) -> Optional[float]:
        value = self._get_value_case_insensitive(data, key)
        if value is None:
            return None
        try:
            return float(value)
        except (TypeError, ValueError):
            omni.log.warn(f"Invalid numeric value '{value}' for key '{key}'.")
            return None

    def _extract_optional_float_by_keys(self, data: Dict[str, object], keys: List[str]) -> Optional[float]:
        for key in keys:
            value = self._extract_optional_float(data, key)
            if value is not None:
                return value
        return None

    @staticmethod
    def _get_value_case_insensitive(data: Dict[str, object], key: str) -> Optional[object]:
        if key in data:
            return data[key]
        lowered = key.lower()
        for existing_key, value in data.items():
            if isinstance(existing_key, str) and existing_key.lower() == lowered:
                return value
        return None

    def _create_procedural_floor(
        self,
        stage,
        root_prim_path: str,
        object_data: Dict[str, object],
    ) -> Optional[str]:
        """
        JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åºŠã‚’æ‰‹ç¶šãçš„ã«ç”Ÿæˆã—ã¾ã™ã€‚

        Args:
            stage: USDã‚¹ãƒ†ãƒ¼ã‚¸
            root_prim_path: è¦ªPrimã®ãƒ‘ã‚¹
            object_data: JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆLength, Width, X, Z ã‚’å«ã‚€ï¼‰

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸåºŠPrimã®ãƒ‘ã‚¹ã¾ãŸã¯None
        """
        return self._floor_generator.generate(stage, root_prim_path, object_data)

    def _create_procedural_walls(
        self,
        stage,
        root_prim_path: str,
        area_size_x: float,
        area_size_y: float,
        door_objects: List[Dict[str, object]] = None,
        origin_mode: str = "center",
    ) -> List[str]:
        """
        éƒ¨å±‹ã®ã‚µã‚¤ã‚ºã«åŸºã¥ã„ã¦4ã¤ã®å£ã‚’æ‰‹ç¶šãçš„ã«ç”Ÿæˆã—ã¾ã™ã€‚
        ãƒ‰ã‚¢ã®ä½ç½®ãŒã‚ã‚‹å ´åˆã¯ã€ãã®éƒ¨åˆ†ã‚’åˆ‡ã‚ŠæŠœãã¾ã™ã€‚

        Args:
            stage: USDã‚¹ãƒ†ãƒ¼ã‚¸
            root_prim_path: è¦ªPrimã®ãƒ‘ã‚¹
            area_size_x: éƒ¨å±‹ã®Xæ–¹å‘ã®ã‚µã‚¤ã‚ºï¼ˆãƒ¡ãƒ¼ãƒˆãƒ«å˜ä½ï¼‰
            area_size_y: éƒ¨å±‹ã®Yæ–¹å‘ã®ã‚µã‚¤ã‚ºï¼ˆãƒ¡ãƒ¼ãƒˆãƒ«å˜ä½ï¼‰
            door_objects: ãƒ‰ã‚¢ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆåˆ‡ã‚ŠæŠœãç”¨ï¼‰
            origin_mode: "center" or "bottom_left"

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå£Primã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        return self._wall_generator.generate_walls(
            stage, root_prim_path, area_size_x, area_size_y, door_objects, origin_mode
        )

    @staticmethod
    def _dedupe_openings(
        openings: List[Dict[str, object]], tol: float = 1e-3
    ) -> List[Dict[str, object]]:
        deduped: List[Dict[str, object]] = []
        seen = set()
        for opening in openings or []:
            try:
                x = float(opening.get("X", 0.0))
                y = float(opening.get("Y", 0.0))
                width = float(opening.get("Width", 0.0))
                height = float(opening.get("Height", 0.0))
                sill = float(opening.get("SillHeight", 0.0))
            except (TypeError, ValueError):
                deduped.append(opening)
                continue
            opening_type = str(opening.get("type", "") or "").lower()
            key = (
                opening_type,
                round(x / tol) * tol,
                round(y / tol) * tol,
                round(width / tol) * tol,
                round(height / tol) * tol,
                round(sill / tol) * tol,
            )
            if key in seen:
                continue
            seen.add(key)
            deduped.append(opening)
        return deduped

    @staticmethod
    def _edge_key(
        p0: Tuple[float, float], p1: Tuple[float, float], tol: float = 1e-3
    ) -> Tuple[Tuple[float, float], Tuple[float, float]]:
        def quantize(point: Tuple[float, float]) -> Tuple[float, float]:
            return (round(point[0] / tol) * tol, round(point[1] / tol) * tol)

        a = quantize(p0)
        b = quantize(p1)
        return tuple(sorted((a, b)))

    def _collect_room_edges(
        self, rooms: List[Dict[str, object]], tol: float = 1e-3
    ) -> Tuple[List[Dict[str, object]], List[Dict[str, object]], List[Dict[str, object]]]:
        edge_map: Dict[Tuple[Tuple[float, float], Tuple[float, float]], Dict[str, object]] = {}
        for room in rooms or []:
            polygon = room.get("room_polygon")
            points = self._wall_generator._normalize_polygon_points(polygon)
            if len(points) < 3:
                continue
            edges = self._wall_generator._build_edges(points)
            for edge in edges:
                key = self._edge_key(edge["start"], edge["end"], tol)
                if key in edge_map:
                    edge_map[key]["count"] += 1
                else:
                    edge_map[key] = {"edge": edge, "count": 1}

        all_edges = [entry["edge"] for entry in edge_map.values()]
        interior_edges = [entry["edge"] for entry in edge_map.values() if entry["count"] > 1]
        exterior_edges = [entry["edge"] for entry in edge_map.values() if entry["count"] == 1]
        return all_edges, interior_edges, exterior_edges

    @staticmethod
    def _edge_axis_signature(
        edge: Dict[str, object], tol: float = 1e-4
    ) -> Optional[Tuple[str, float, float, float]]:
        p0 = edge.get("start")
        p1 = edge.get("end")
        if not p0 or not p1:
            return None
        x0, y0 = p0
        x1, y1 = p1
        if abs(x0 - x1) < tol:
            x = round(x0 / tol) * tol
            y_min, y_max = sorted([y0, y1])
            return ("v", x, y_min, y_max)
        if abs(y0 - y1) < tol:
            y = round(y0 / tol) * tol
            x_min, x_max = sorted([x0, x1])
            return ("h", y, x_min, x_max)
        return None

    @staticmethod
    def _build_edge_from_points(
        p0: Tuple[float, float], p1: Tuple[float, float]
    ) -> Optional[Dict[str, object]]:
        dx = p1[0] - p0[0]
        dy = p1[1] - p0[1]
        length = math.hypot(dx, dy)
        if length <= 1e-6:
            return None
        unit = (dx / length, dy / length)
        angle = math.degrees(math.atan2(dy, dx))
        return {
            "start": (float(p0[0]), float(p0[1])),
            "end": (float(p1[0]), float(p1[1])),
            "unit": unit,
            "length": float(length),
            "angle": float(angle),
        }

    def _merge_axis_aligned_edges(
        self, edges: List[Dict[str, object]], tol: float = 1e-4
    ) -> List[Dict[str, object]]:
        from collections import defaultdict

        horiz = defaultdict(list)
        vert = defaultdict(list)
        others: List[Dict[str, object]] = []

        for edge in edges or []:
            sig = self._edge_axis_signature(edge, tol)
            if not sig:
                others.append(edge)
                continue
            axis, coord, s0, s1 = sig
            if axis == "h":
                horiz[coord].append((s0, s1))
            else:
                vert[coord].append((s0, s1))

        def merge_ranges(ranges: List[Tuple[float, float]]) -> List[Tuple[float, float]]:
            ranges = sorted(ranges, key=lambda r: r[0])
            merged: List[List[float]] = []
            for start, end in ranges:
                if not merged or start > merged[-1][1] + tol:
                    merged.append([start, end])
                else:
                    merged[-1][1] = max(merged[-1][1], end)
            return [(r[0], r[1]) for r in merged]

        merged_edges: List[Dict[str, object]] = []
        for y, ranges in horiz.items():
            for x0, x1 in merge_ranges(ranges):
                edge = self._build_edge_from_points((x0, y), (x1, y))
                if edge:
                    merged_edges.append(edge)

        for x, ranges in vert.items():
            for y0, y1 in merge_ranges(ranges):
                edge = self._build_edge_from_points((x, y0), (x, y1))
                if edge:
                    merged_edges.append(edge)

        merged_edges.extend(others)
        return merged_edges

    @staticmethod
    def _edge_overlaps_any(
        sig: Tuple[str, float, float, float],
        outer_sigs: List[Tuple[str, float, float, float]],
        tol: float = 1e-4,
    ) -> bool:
        axis, coord, s0, s1 = sig
        for osig in outer_sigs:
            if osig[0] != axis:
                continue
            if abs(osig[1] - coord) > tol:
                continue
            o0, o1 = osig[2], osig[3]
            if min(s1, o1) - max(s0, o0) > tol:
                return True
        return False

    def _filter_edges_against_outer(
        self,
        edges: List[Dict[str, object]],
        outer_edges: List[Dict[str, object]],
        tol: float = 1e-4,
    ) -> List[Dict[str, object]]:
        outer_sigs = [self._edge_axis_signature(edge, tol) for edge in outer_edges or []]
        outer_sigs = [sig for sig in outer_sigs if sig]
        if not outer_sigs:
            return edges

        filtered: List[Dict[str, object]] = []
        for edge in edges or []:
            sig = self._edge_axis_signature(edge, tol)
            if sig and self._edge_overlaps_any(sig, outer_sigs, tol):
                continue
            filtered.append(edge)
        return filtered

    def _normalize_furniture_name(self, json_name: str) -> str:
        """
        AIãŒç”Ÿæˆã—ãŸJSONã®å®¶å…·åã‚’ã€USD SearchãŒç†è§£ã§ãã‚‹
        æ­£è¦åŒ–ã•ã‚ŒãŸæ¤œç´¢ã‚¯ã‚¨ãƒªã«å¤‰æ›ã—ã¾ã™ã€‚
        """

        # 1. æ—¢çŸ¥ã®åå‰ã‚’ãƒžãƒƒãƒ”ãƒ³ã‚°
        #    (ã“ã®è¾žæ›¸ã¯å¿…è¦ã«å¿œã˜ã¦æ‹¡å¼µã—ã¦ãã ã•ã„)
        name_mapping = {
            # ãƒ™ãƒƒãƒ‰
            "accessible_master_bed": "bed",
            "left_side_access_bed": "bed",
            "master_bed": "bed",
            # å†·è”µåº«
            "compact_refrigerator": "refrigerator",
            "tall_refrigerator": "refrigerator",
            # æ¤…å­
            "support_chair": "chair",
            "office_chair": "chair",
            "upper_corner_chair": "chair",
            "transfer_chair": "chair",
            # ãƒ†ãƒ¬ãƒ“
            "wall_mounted_tv": "television",
            "left_wall_wallmount_tv": "television",
            "slim_wall_tv": "television",
            "wall_tv": "television",
            # å¼•ãå‡ºã—ãƒ»åŽç´
            "accessible_drawer": "drawer",
            "lower_corner_drawer": "drawer",
            "low_drawer": "drawer",
            # ãƒ‰ã‚¢
            "main_entrance_door": "door",
            "entrance_door": "door",
            "inward_swing_door": "door",
            "swing_door": "door",
            # ãã®ä»–
            "office_table_1": "office table",
            "office_cabinet": "cabinet",
            "office_lamp": "lamp",
            "office_printer": "printer",
            "single_spiral_notepad": "notepad",
        }

        normalized_name = json_name.lower().replace("_", " ")

        # ãƒžãƒƒãƒ”ãƒ³ã‚°è¾žæ›¸ï¼ˆã‚­ãƒ¼ã‚’å°æ–‡å­—åŒ–ãƒ»ã‚¹ãƒšãƒ¼ã‚¹åŒ–ã—ã¦ï¼‰ã§æ¤œç´¢
        for key, value in name_mapping.items():
            if key.replace("_", " ") in normalized_name:
                omni.log.info(f"Normalized '{json_name}' -> '{value}' (using mapping)")
                return value

        # 2. ãƒžãƒƒãƒ”ãƒ³ã‚°ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        #    ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’é™¤åŽ»ã—ã¦æœ€å¾Œã®å˜èªžã‚’ä½¿ç”¨
        words = normalized_name.split()
        if words:
            fallback_query = words[-1]
            omni.log.info(f"Normalized '{json_name}' -> '{fallback_query}' (using fallback)")
            return fallback_query

        omni.log.warn(f"Could not normalize '{json_name}', using original.")
        return json_name

    def _build_search_query_from_object(self, obj: Dict[str, object]) -> str:
        """
        JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ1ä»¶ã‹ã‚‰ã‚»ãƒžãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ç”¨ã®ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—ã‚’çµ„ã¿ç«‹ã¦ã‚‹ã€‚

        å„ªå…ˆé †:
        1) search_prompt ãŒã‚ã‚Œã°ãã‚Œã‚’ãƒ™ãƒ¼ã‚¹ã«ä½¿ç”¨
        2) search_prompt ãŒç©ºã§ object_name ãŒã‚ã‚Œã° _normalize_furniture_name ã§æ­£è¦åŒ–
        3) ã©ã¡ã‚‰ã‚‚ç„¡ã‘ã‚Œã°æ±Žç”¨ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’ä½¿ã†
        category ãŒã‚ã‚Œã°å…ˆé ­ã«ä»˜ä¸Žã™ã‚‹ã€‚
        """
        object_name = str(obj.get("object_name", "") or "").strip()
        category = str(obj.get("category", "") or "").strip()
        search_prompt = str(obj.get("search_prompt", "") or "").strip()

        if search_prompt:
            base_text = search_prompt
        elif object_name:
            base_text = self._normalize_furniture_name(object_name)
        else:
            base_text = "generic furniture object"

        if category:
            query = f"{category}, {base_text}"
        else:
            query = base_text

        omni.log.info(
            f"[SearchQuery] category='{category}', name='{object_name}', "
            f"search_prompt_len={len(search_prompt)}, query='{query}'"
        )
        return query

    @staticmethod
    def _compute_scale_with_up_axis(
        up_axis: str,
        original_sizes: Tuple[float, float, float],
        target_sizes: Tuple[float, float, float],
        min_size: float = 1e-6,
    ) -> Tuple[float, float, float]:
        """
        upAxisã«å¿œã˜ãŸã‚¹ã‚±ãƒ¼ãƒ«æ¯”ã‚’è¨ˆç®—ã™ã‚‹ã€‚

        Z-upã‚¹ãƒ†ãƒ¼ã‚¸ã§ã®æƒ³å®š:
        - Z-upã‚¢ã‚»ãƒƒãƒˆ: (Xâ†’X, Yâ†’Y, Zâ†’Z)
        - Y-upã‚¢ã‚»ãƒƒãƒˆ: RotateX(+90) ã‚’é©ç”¨ã—ã¦Z-upã«è£œæ­£ã™ã‚‹ãŸã‚ã€(Xâ†’X, Zâ†’Y, Yâ†’Z) ã‚’è€ƒæ…®
        """
        original_x, original_y, original_z = original_sizes
        target_x, target_y, target_z = target_sizes

        def safe_div(num: float, denom: float) -> float:
            if abs(denom) > min_size and num > 0:
                return num / denom
            return 1.0

        if up_axis == "Y":
            # RotateX(+90) ã«ã‚ˆã‚Š local Y -> world Z, local Z -> world Y
            scale_x = safe_div(target_x, original_x)
            scale_y = safe_div(target_z, original_y)
            scale_z = safe_div(target_y, original_z)
        else:
            scale_x = safe_div(target_x, original_x)
            scale_y = safe_div(target_y, original_y)
            scale_z = safe_div(target_z, original_z)

        return scale_x, scale_y, scale_z

    def _get_asset_up_axis(self, asset_url: str) -> str:
        """
        ã‚¢ã‚»ãƒƒãƒˆã®upAxisã‚’å–å¾—ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã€‚æœªçŸ¥ãƒ»å–å¾—å¤±æ•—æ™‚ã¯'Y'ã‚’è¿”ã™ã€‚
        """
        if not asset_url:
            omni.log.warn("[UpAxis] asset_url is empty. Fallback to 'Y'.")
            return "Y"

        if not hasattr(self, "_asset_up_axis_cache"):
            self._asset_up_axis_cache = {}

        cached = self._asset_up_axis_cache.get(asset_url)
        if cached:
            omni.log.info(f"[UpAxis] Cache hit: {asset_url} -> {cached}")
            return cached

        up_axis = "Y"
        try:
            stage = Usd.Stage.Open(asset_url)
            if not stage:
                omni.log.warn(f"[UpAxis] Failed to open stage for {asset_url}. Fallback to 'Y'.")
            else:
                axis_token = UsdGeom.GetStageUpAxis(stage)
                up_axis = "Z" if axis_token == UsdGeom.Tokens.z else "Y"
                omni.log.info(f"[UpAxis] Retrieved upAxis='{up_axis}' for {asset_url}")
        except Exception as exc:
            omni.log.warn(f"[UpAxis] Error while getting upAxis for {asset_url}: {exc}. Fallback to 'Y'.")

        self._asset_up_axis_cache[asset_url] = up_axis
        return up_axis

    async def _get_asset_size(self, asset_url: str) -> Optional[int]:
        if not asset_url:
            return None
        normalized = self._normalize_asset_url(asset_url)
        cache = getattr(self, "_asset_size_cache", None)
        if cache is None:
            self._asset_size_cache = {}
            cache = self._asset_size_cache
        if normalized in cache:
            return cache[normalized]
        try:
            import omni.client
            result, entry = await omni.client.stat_async(normalized)
            if result == omni.client.Result.OK and entry:
                size = int(entry.size) if entry.size is not None else None
            else:
                size = None
        except Exception as exc:
            omni.log.warn(f"[AssetSize] Failed to stat '{normalized}': {exc}")
            size = None
        cache[normalized] = size
        return size

    async def _get_asset_identity_key(
        self, asset_url: str, size_bytes: Optional[int] = None
    ) -> Optional[str]:
        if not asset_url:
            return None
        if size_bytes is None:
            size_bytes = await self._get_asset_size(asset_url)
        return self._build_asset_identity_key(asset_url, size_bytes)

    async def _semantic_search_asset(
        self,
        query: str,
        search_root: str,
        exclude_urls: Optional[set] = None,
        exclude_keys: Optional[set] = None,
    ) -> Optional[str]:
        """
        å¤–éƒ¨ã‚»ãƒžãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢API (Vector Search) ã‚’ä½¿ç”¨ã—ã¦ã‚¢ã‚»ãƒƒãƒˆã‚’æ¤œç´¢ã—ã¾ã™ã€‚
        GETã§ã¯ãªãPOSTã‚’ä½¿ç”¨ã—ã€CLIPåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«ã‚ˆã‚‹ç›´æŽ¥æ¤œç´¢ã‚’è¡Œã„ã¾ã™ã€‚

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª (ä¾‹: "A comfortable chair")
            search_root: æ¤œç´¢ãƒ«ãƒ¼ãƒˆURLï¼ˆä»Šå›žã¯ä½¿ç”¨ã—ãªã„ãŒäº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
        Returns:
            ã‚¢ã‚»ãƒƒãƒˆã®Omniverse URLã€ã¾ãŸã¯è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
        """
        import requests
        import asyncio
        import json

        # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š
        api_url = "http://192.168.11.65:30080/search"
        api_basic_auth = "omniverse:tsukuverse"

        # --- Vector Searchç”¨ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰æ§‹ç¯‰ ---
        payload = {
            "vector_queries": [
                {
                    "field_name": "clip-embedding.embedding",
                    "query_type": "text",
                    "query": query,
                }
            ],
            "return_metadata": True,
            "return_images": False,
            "limit": VECTOR_SEARCH_LIMIT,
            "file_extension_include": "usd,usda,usdc,usdz",
            "file_extension_exclude": "png,jpg,jpeg",
        }

        omni.log.info(f"[Vector Search] POST Request to: {api_url}")
        omni.log.info(f"  Payload: {json.dumps(payload, ensure_ascii=False)}")

        try:
            # åŒæœŸå‡¦ç†ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œï¼ˆUIã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
            loop = asyncio.get_event_loop()

            def sync_request():
                user, password = api_basic_auth.split(":", 1)
                response = requests.post(api_url, json=payload, auth=(user, password), timeout=60)
                response.raise_for_status()
                return response.json()

            result = await loop.run_in_executor(None, sync_request)

            omni.log.info("[Vector Search] API Response received")
            omni.log.info(f"  Response: {json.dumps(result, ensure_ascii=False)[:500]}")

            # --- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰USDãƒ‘ã‚¹ã‚’æŠ½å‡º ---
            items = []
            if isinstance(result, list):
                items = result
            elif isinstance(result, dict):
                if "results" in result:
                    items = result["results"]
                elif "hits" in result:
                    if isinstance(result["hits"], dict) and "hits" in result["hits"]:
                        items = result["hits"]["hits"]
                elif "items" in result:
                    items = result["items"]

            if not items:
                omni.log.warn(f"[Vector Search] No results found for '{query}'")
                return None

            usd_extensions = (".usd", ".usda", ".usdc", ".usdz")
            excluded_urls = set()
            for url in getattr(self, "_asset_blacklist", set()):
                if url:
                    excluded_urls.add(self._normalize_asset_url(str(url)))
            if exclude_urls:
                for url in exclude_urls:
                    if url:
                        excluded_urls.add(self._normalize_asset_url(str(url)))

            excluded_keys = set(getattr(self, "_asset_blacklist_keys", set()))
            if exclude_keys:
                for key in exclude_keys:
                    if key:
                        excluded_keys.add(str(key))
            seen_keys = set()

            def _coerce_size(value) -> Optional[int]:
                try:
                    size_val = int(value)
                    return size_val if size_val >= 0 else None
                except (TypeError, ValueError):
                    return None

            def _extract_size(data: object) -> Optional[int]:
                if not isinstance(data, dict):
                    return None
                for key in ("size", "file_size", "fileSize", "bytes", "file_size_bytes", "content_length"):
                    if key in data:
                        size_val = _coerce_size(data.get(key))
                        if size_val is not None:
                            return size_val
                nested = data.get("stat")
                if isinstance(nested, dict):
                    for key in ("size", "file_size", "bytes"):
                        if key in nested:
                            size_val = _coerce_size(nested.get(key))
                            if size_val is not None:
                                return size_val
                return None

            candidates = []
            for item in items:
                item_data = item.get("_source", item) if isinstance(item, dict) else {}
                candidate_keys = ["uri", "url", "path", "file_path", "asset_path"]
                size_val = _extract_size(item_data) or _extract_size(item)
                for key in candidate_keys:
                    if isinstance(item_data, dict) and key in item_data:
                        val = item_data[key]
                        if val and isinstance(val, str):
                            if val.lower().endswith(usd_extensions):
                                candidates.append((val, size_val))
                                break
                            omni.log.info(f"[Vector Search] Skipping non-USD file: {val}")

            if not candidates:
                omni.log.warn(f"[Vector Search] No USD files found in results for '{query}'")
                return None

            for candidate_url, candidate_size in candidates:
                normalized_url = self._normalize_asset_url(candidate_url)
                if normalized_url in excluded_urls:
                    omni.log.info(f"[Vector Search] Skipping blacklisted asset: {normalized_url}")
                    continue

                size_bytes = candidate_size
                if size_bytes is None:
                    size_bytes = await self._get_asset_size(normalized_url)

                identity_key = self._build_asset_identity_key(normalized_url, size_bytes)
                if identity_key and identity_key in excluded_keys:
                    omni.log.info(
                        f"[Vector Search] Skipping blacklisted key: {identity_key} ({normalized_url})"
                    )
                    continue
                if identity_key and identity_key in seen_keys:
                    continue
                if identity_key:
                    seen_keys.add(identity_key)

                omni.log.info(f"[Vector Search] Final Asset URL: {normalized_url}")
                return normalized_url

            omni.log.warn(f"[Vector Search] No USD files found after filtering for '{query}'")
            return None

        except requests.exceptions.RequestException as exc:
            omni.log.error(f"[Vector Search] HTTP request failed: {exc}")
            return None
        except Exception as exc:
            omni.log.error(f"[Vector Search] Unexpected error: {exc}")
            import traceback
            omni.log.error(f"Stack trace:\n{traceback.format_exc()}")
            return None

    def _save_generation_log(
        self,
        step1_stats: Dict[str, Any],
        step2_stats: Dict[str, Any],
        analysis_text: str,
        layout_json: Dict[str, Any],
        collision_results: List[Tuple[str, str, float]],
        model_name: str,
        step2_model_name: str,
        ai_overrides: Dict[str, Any],
        image_path: str,
        prompt1_text: str,
        prompt2_text: str,
        dimensions_text: str,
        tag: str = "full"
    ) -> Optional[str]:
        """AIç”Ÿæˆã®å…¨è¡Œç¨‹ã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹"""
        try:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            log_dir = self._get_log_dir()
            os.makedirs(log_dir, exist_ok=True)
            
            log_filename = f"{timestamp}_{tag}.log"
            log_path = os.path.join(log_dir, log_filename)

            total_time = (step1_stats.get("time", 0) or 0) + (step2_stats.get("time", 0) or 0)
            total_prompt = (step1_stats.get("prompt_tokens", 0) or 0) + (step2_stats.get("prompt_tokens", 0) or 0)
            total_completion = (step1_stats.get("completion_tokens", 0) or 0) + (step2_stats.get("completion_tokens", 0) or 0)
            total_tokens = (step1_stats.get("total_tokens", 0) or 0) + (step2_stats.get("total_tokens", 0) or 0)

            lines = []
            lines.append("AI Layout Generation Log")
            lines.append(f"timestamp: {timestamp}")
            lines.append("")
            lines.append("[AI Settings]")
            lines.append(f"step1_model: {model_name}")
            lines.append(f"step2_model: {step2_model_name}")
            for k, v in ai_overrides.items():
                lines.append(f"{k}: {v}")
            lines.append("")
            
            lines.append("[Step 1 Stats]")
            lines.append(f"time_sec: {step1_stats.get('time', 0)}")
            lines.append(f"input_tokens: {step1_stats.get('prompt_tokens', 0)}")
            lines.append(f"output_tokens: {step1_stats.get('completion_tokens', 0)}")
            lines.append(f"total_tokens: {step1_stats.get('total_tokens', 0)}")
            lines.append("")

            lines.append("[Step 2 Stats]")
            lines.append(f"time_sec: {step2_stats.get('time', 0)}")
            lines.append(f"input_tokens: {step2_stats.get('prompt_tokens', 0)}")
            lines.append(f"output_tokens: {step2_stats.get('completion_tokens', 0)}")
            lines.append(f"total_tokens: {step2_stats.get('total_tokens', 0)}")
            lines.append("")

            lines.append("[Total Stats]")
            lines.append(f"time_sec: {total_time}")
            lines.append(f"input_tokens: {total_prompt}")
            lines.append(f"output_tokens: {total_completion}")
            lines.append(f"total_tokens: {total_tokens}")
            lines.append("")

            lines.append("[Analysis Text]")
            lines.append(analysis_text)
            lines.append("")

            lines.append("[AABB Collisions]")
            if collision_results:
                for name1, name2, volume in collision_results:
                    lines.append(f"- {name1} vs {name2}: overlap_volume={volume}")
            else:
                lines.append("No collisions detected.")
            lines.append("")

            lines.append("[Room Size]")
            lines.append(f"area_size_X: {layout_json.get('area_size_X', 'N/A')}")
            lines.append(f"area_size_Y: {layout_json.get('area_size_Y', 'N/A')}")
            lines.append("")

            lines.append("[Inputs]")
            lines.append(f"image_path: {image_path}")
            lines.append("")

            lines.append("[dimensions.txt]")
            lines.append(dimensions_text)
            lines.append("")

            lines.append("[prompt1]")
            lines.append(prompt1_text)
            lines.append("")

            lines.append("[prompt2]")
            lines.append(prompt2_text)
            lines.append("")

            with open(log_path, "w", encoding="utf-8") as f:
                f.write("\n".join(lines))
            
            omni.log.info(f"Detailed generation log saved to: {log_path}")
            return log_path

        except Exception as e:
            omni.log.error(f"Failed to save generation log: {e}")
            import traceback
            omni.log.error(traceback.format_exc())
            return None
